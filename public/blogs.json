{"status":"ok","feed":{"url":"https://medium.com/feed/@arjun.anand1","title":"Stories by Arjun Anandkumar on Medium","link":"https://medium.com/@arjun.anand1?source=rss-e4abfb938c21------2","author":"","description":"Stories by Arjun Anandkumar on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*YzFULlDxmY-hj9ri"},"items":[{"title":"Flexibility in your data ingestion stack","pubDate":"2025-07-29 08:37:04","link":"https://medium.com/@arjun.anand1/flexibility-in-your-data-ingestion-stack-f1f15cde8888?source=rss-e4abfb938c21------2","guid":"https://medium.com/p/f1f15cde8888","author":"Arjun Anandkumar","thumbnail":"","description":"\n<p>Spoiler alert: This one is yet again about one of my favorite finds in the past year, dlt from\u00a0<a href=\"https://dlthub.com/\">dlthub</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/928/1*KFOG6ygJJd13BbHn1no8Gg.png\"><figcaption>dlt and its ecosystem</figcaption></figure><h3>Background</h3>\n<p>As a data engineer, if you\u2019ve ever sat around with one of those low-code, inflexible tools, trying your best to bend it to fit your needs\u200a\u2014\u200ayou know the pain. Our data ingestion needs are growing, and the complexity is growing alongside (maybe even at a faster\u00a0rate).</p>\n<p>I\u2019ve written about my experience with using dlt for ingesting data from REST APIs <a href=\"https://medium.com/@arjun.anand1/rest-api-ingestions-made-easy-with-dlt-d77b2bf1d50b\">here</a>, which also cover some of the basics that one needs to know about dlt. The fun part is that it fundamentally remains the same, and really, the only thing that changes is the kind of\u00a0source.</p>\n<p>Recently, we had an interesting data source that we needed to get data out of. On paper, it was just an everyday kind of task: Ingest data from an MSSQL database. But the reality was just something else.</p>\n<p>The specific table from the database that we needed to get data out of was collecting data from a large set of meters, and had data volumes in the 300 billion range, with the daily increment being around 60 million-ish records. The interesting thing being how we get data out of it. It wasn\u2019t organised the best way and is also not <em>performant</em> at this point. The base table had nothing that we could do an incremental load on (CDC was out of the question and denied a long time ago), but we could do so if we joined with another table that collected deltas which had a rowversion in binary format (which one could do <strong><em>MAX()</em></strong> on), updated another table to inform the source system owners that we had read until a certain rowversion, and so\u00a0on.</p>\n<p>The mandate was to start ingesting data from the beginning of 2020. That\u2019s about 160 billion records to date. The initial load would take a crazy amount of time as a regular pipeline, so I put together a script with polars that would do an initial dump of parquet files, that we could later just import into Snowflake and use as our base table. Lucky for us, dlt does support loading of data into an existing table\u200a\u2014\u200aperfect, because the deltas would typically also contain updates that we need to merge. The config is rather simple, all I had to do was alter the table and add a couple of metadata columns, as described <a href=\"https://dlthub.com/docs/general-usage/destination-tables#loading-data-into-existing-tables-not-created-by-dlt\">here</a>.</p>\n<p>Now that I had my base data ready, and all I had to do is set up the dlt pipeline to load data from the last delta which we already updated in the table that tracks what rowversion we had read\u00a0until.</p>\n<p>In essence, this is what we needed to do each time we ingest\u00a0data:</p>\n<p>\u2014 Step 1: Get the max RowVersion into a temp table.<br>\u200a\u2014\u200aStep 2: Update the working RowVersion in the tracking table<br>\u200a\u2014\u200aStep 3: Create a temp table with ID and TIMESTAMP<br>\u200a\u2014\u200aStep 4: Join with TimeSeries table to fetch full records<br>\u200a\u2014\u200aStep 5: Clean up temp\u00a0table</p>\n<p>Temp tables is something I prefer to avoid if I can, so I rewrote these to be CTEs instead\u200a\u2014\u200aexcept the updates. That reduced this to basically a single step query where I combine steps 3 and 4. Great, base query sorted, onto the\u00a0rest.</p>\n<h3>The pipeline begins to take\u00a0shape</h3>\n<p>Since the basis of this ingestion is to first obtain the MAX(rowversion), I re-used the existing connection string I had and used that to create an SQLAlchemy engine to fetch this information, and save it as a variable to update the tracking table and perform the final update post-ingestion.</p>\n<pre>engine = sa.create_engine(connection_string)<br>with engine.begin() as conn:<br>    logger.info(\"Fetching max rowversion\")<br>    result = conn.execute(<br>        sa.text(\"\"\"<br>        SELECT MAX([RowVersion]) AS max_rowversion<br>        FROM [db].[TimeSeriesValuesDelta]<br>    \"\"\")<br>    )<br>    row = result.fetchone()<br>    rowversion_working = (<br>        \"0x\" + row.max_rowversion.hex().upper()<br>    ) </pre>\n<p>Then, I update the tracking table with this information:</p>\n<pre>with engine.begin() as conn:<br>    update_sql = f\"\"\"<br>        UPDATE A<br>        SET A.ROWVERSION_WORKING = CONVERT(binary(8), {rowversion_working}, 1)<br>        FROM db.TimeSeriesValuesDelta_SLET A<br>        WHERE [SYSTEM] = 'Snowflake_SYNC'<br>        \"\"\"<br><br>    result = conn.execute(sa.text(update_sql))<br>    logger.info(f\"Rows updated: {result.rowcount}\")</pre>\n<p>With that, I have everything I need from a setup perspective, and then setup my dlt source, and the query adapter callback (more on this in a bit). Also, see <a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database/setup\">this</a> page on an end-to-end source setup for databases.</p>\n<pre>def query_adapter_callback(query, table):<br>    filtered_query = \"\"\"<br>        WITH TSSync_TEMP AS (<br>            SELECT A.ID, A.TIMESTAMP <br>            FROM db.TimeSeriesValuesDelta A<br>            INNER JOIN db.TimeSeriesValuesDelta_SLET B<br>                ON A.RowVersion BETWEEN B.MAX_ROWVERSION_IMPORTED AND B.ROWVERSION_WORKING<br>            WHERE B.SYSTEM = 'Snowflake_SYNC'<br>        )<br>        SELECT ts.*<br>        FROM db.TimeSeriesValues ts<br>        INNER JOIN TSSync_TEMP tmp<br>            ON tmp.ID = ts.ID<br>            AND tmp.TIMESTAMP = ts.TIMESTAMP<br>        \"\"\"<br>    logger.info(f\"Executing query:\\n{filtered_query}\")<br>    return sa.text(filtered_query)<br><br>#########################################<br>##### dlt Pipeline and source setup #####<br>#########################################<br>source = sql_database(<br>    table_names=[<br>        \"TimeSeriesValues\"<br>    ],<br>    credentials=connection_string,<br>    backend=\"sqlalchemy\",<br>    query_adapter_callback=query_adapter_callback,<br>    schema=\"db\",<br>    chunk_size=10000000,<br>)<br>source.TimeSeriesValues.apply_hints(<br>    table_name=\"customname_timeseriesvalues\",<br>    columns={<br>        \"TIMESTAMP\": {\"data_type\": \"timestamp\", \"precision\": 9, \"timezone\": False}<br>    },  # Preserve TIMESTAMP_NTZ on original data.<br>)<br><br>pipeline.run(<br>    source, write_disposition=\"merge\", primary_key=[\"ID\", \"TIMESTAMP\"]<br>)</pre>\n<p>The source setup here is quite ordinary except for the query adapter callback. The hint is to just to have the final table name in Snowflake as <em>customname</em>_timeseriesvalues, and the columns hint is to make sure that dlt does not interpret the TIMESTAMP column to be of type TIMESTAMP_TZ which does not match the base\u00a0data.</p>\n<p>What thequery adapter callback does here for us is quite awesome. By default, dlt ingests all of the records from the source table. However, by using query_adapter_callback, it is possible to return a custom query that dlt should use instead for fetching data. This allowed me to return my desired CTE as the final query to be run on the\u00a0source.</p>\n<p>Then, all I have to do is run the pipeline with \u201cmerge\u201d as the <a href=\"https://dlthub.com/docs/general-usage/incremental-loading#choosing-a-write-disposition\">write_disposition</a> and define my primary\u00a0keys.</p>\n<p>Finally, provided that the pipeline ran successfully, I just needed to reuse the connection string once again to perform the final\u00a0update:</p>\n<pre>with engine.begin() as conn:<br>    update_sql = f\"\"\"<br>        UPDATE A<br>        SET<br>            A.MAX_ROWVERSION_IMPORTED = CONVERT(binary(8), {rowversion_working}, 1),<br>            DATO = GETDATE()<br>        FROM db.TimeSeriesValuesDelta_SLET A<br>        WHERE [SYSTEM] = 'Snowflake_SYNC';<br>        \"\"\"<br>    result = conn.execute(sa.text(update_sql))<br>    logger.info(f\"Rows updated: {result.rowcount}\")</pre>\n<p>This completes the ingestion, and provides us with the reliable raw data that we can deliver for analytics needs.</p>\n<h3>Conclusion</h3>\n<p>I haven\u2019t used other popular ingestion tools like Fivetran, Airbyte extensively enough to know if this is doable with either of them. I am guessing that PyAirbyte could possibly do the same here\u200a\u2014\u200abut I am always curious to hear takes on tackling these sorts of problems. It certainly was not possible with our other ingestion tool of choice (Talend from\u00a0Qlik).</p>\n<p>While this isn\u2019t the simplest of our dlt pipelines, it\u2019s relatively easy to read as long as the extraction logic is documented (there is no such thing as too much documentation), and it <strong>works</strong>. Now, could I have tacked this along as a python script that just uses polars or something under the hood, and got this done? <em>Absolutely</em>. But will it be just as readable and reliable as any of our other pipelines? <em>Probably\u00a0not</em>.</p>\n<p>Every time a new requirement comes up, I am quite amazed that the dlthub team have thought about these scenarios and built possibilities for custom logic to be implemented where required. After all, it\u2019s just python code, and as long as we feed the data to dlt in a way it can work with, it can run pretty much anything. Kudos, <a href=\"https://medium.com/u/1725370bcbe1\">dlthub.com</a>!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f1f15cde8888\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Spoiler alert: This one is yet again about one of my favorite finds in the past year, dlt from\u00a0<a href=\"https://dlthub.com/\">dlthub</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/928/1*KFOG6ygJJd13BbHn1no8Gg.png\"><figcaption>dlt and its ecosystem</figcaption></figure><h3>Background</h3>\n<p>As a data engineer, if you\u2019ve ever sat around with one of those low-code, inflexible tools, trying your best to bend it to fit your needs\u200a\u2014\u200ayou know the pain. Our data ingestion needs are growing, and the complexity is growing alongside (maybe even at a faster\u00a0rate).</p>\n<p>I\u2019ve written about my experience with using dlt for ingesting data from REST APIs <a href=\"https://medium.com/@arjun.anand1/rest-api-ingestions-made-easy-with-dlt-d77b2bf1d50b\">here</a>, which also cover some of the basics that one needs to know about dlt. The fun part is that it fundamentally remains the same, and really, the only thing that changes is the kind of\u00a0source.</p>\n<p>Recently, we had an interesting data source that we needed to get data out of. On paper, it was just an everyday kind of task: Ingest data from an MSSQL database. But the reality was just something else.</p>\n<p>The specific table from the database that we needed to get data out of was collecting data from a large set of meters, and had data volumes in the 300 billion range, with the daily increment being around 60 million-ish records. The interesting thing being how we get data out of it. It wasn\u2019t organised the best way and is also not <em>performant</em> at this point. The base table had nothing that we could do an incremental load on (CDC was out of the question and denied a long time ago), but we could do so if we joined with another table that collected deltas which had a rowversion in binary format (which one could do <strong><em>MAX()</em></strong> on), updated another table to inform the source system owners that we had read until a certain rowversion, and so\u00a0on.</p>\n<p>The mandate was to start ingesting data from the beginning of 2020. That\u2019s about 160 billion records to date. The initial load would take a crazy amount of time as a regular pipeline, so I put together a script with polars that would do an initial dump of parquet files, that we could later just import into Snowflake and use as our base table. Lucky for us, dlt does support loading of data into an existing table\u200a\u2014\u200aperfect, because the deltas would typically also contain updates that we need to merge. The config is rather simple, all I had to do was alter the table and add a couple of metadata columns, as described <a href=\"https://dlthub.com/docs/general-usage/destination-tables#loading-data-into-existing-tables-not-created-by-dlt\">here</a>.</p>\n<p>Now that I had my base data ready, and all I had to do is set up the dlt pipeline to load data from the last delta which we already updated in the table that tracks what rowversion we had read\u00a0until.</p>\n<p>In essence, this is what we needed to do each time we ingest\u00a0data:</p>\n<p>\u2014 Step 1: Get the max RowVersion into a temp table.<br>\u200a\u2014\u200aStep 2: Update the working RowVersion in the tracking table<br>\u200a\u2014\u200aStep 3: Create a temp table with ID and TIMESTAMP<br>\u200a\u2014\u200aStep 4: Join with TimeSeries table to fetch full records<br>\u200a\u2014\u200aStep 5: Clean up temp\u00a0table</p>\n<p>Temp tables is something I prefer to avoid if I can, so I rewrote these to be CTEs instead\u200a\u2014\u200aexcept the updates. That reduced this to basically a single step query where I combine steps 3 and 4. Great, base query sorted, onto the\u00a0rest.</p>\n<h3>The pipeline begins to take\u00a0shape</h3>\n<p>Since the basis of this ingestion is to first obtain the MAX(rowversion), I re-used the existing connection string I had and used that to create an SQLAlchemy engine to fetch this information, and save it as a variable to update the tracking table and perform the final update post-ingestion.</p>\n<pre>engine = sa.create_engine(connection_string)<br>with engine.begin() as conn:<br>    logger.info(\"Fetching max rowversion\")<br>    result = conn.execute(<br>        sa.text(\"\"\"<br>        SELECT MAX([RowVersion]) AS max_rowversion<br>        FROM [db].[TimeSeriesValuesDelta]<br>    \"\"\")<br>    )<br>    row = result.fetchone()<br>    rowversion_working = (<br>        \"0x\" + row.max_rowversion.hex().upper()<br>    ) </pre>\n<p>Then, I update the tracking table with this information:</p>\n<pre>with engine.begin() as conn:<br>    update_sql = f\"\"\"<br>        UPDATE A<br>        SET A.ROWVERSION_WORKING = CONVERT(binary(8), {rowversion_working}, 1)<br>        FROM db.TimeSeriesValuesDelta_SLET A<br>        WHERE [SYSTEM] = 'Snowflake_SYNC'<br>        \"\"\"<br><br>    result = conn.execute(sa.text(update_sql))<br>    logger.info(f\"Rows updated: {result.rowcount}\")</pre>\n<p>With that, I have everything I need from a setup perspective, and then setup my dlt source, and the query adapter callback (more on this in a bit). Also, see <a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database/setup\">this</a> page on an end-to-end source setup for databases.</p>\n<pre>def query_adapter_callback(query, table):<br>    filtered_query = \"\"\"<br>        WITH TSSync_TEMP AS (<br>            SELECT A.ID, A.TIMESTAMP <br>            FROM db.TimeSeriesValuesDelta A<br>            INNER JOIN db.TimeSeriesValuesDelta_SLET B<br>                ON A.RowVersion BETWEEN B.MAX_ROWVERSION_IMPORTED AND B.ROWVERSION_WORKING<br>            WHERE B.SYSTEM = 'Snowflake_SYNC'<br>        )<br>        SELECT ts.*<br>        FROM db.TimeSeriesValues ts<br>        INNER JOIN TSSync_TEMP tmp<br>            ON tmp.ID = ts.ID<br>            AND tmp.TIMESTAMP = ts.TIMESTAMP<br>        \"\"\"<br>    logger.info(f\"Executing query:\\n{filtered_query}\")<br>    return sa.text(filtered_query)<br><br>#########################################<br>##### dlt Pipeline and source setup #####<br>#########################################<br>source = sql_database(<br>    table_names=[<br>        \"TimeSeriesValues\"<br>    ],<br>    credentials=connection_string,<br>    backend=\"sqlalchemy\",<br>    query_adapter_callback=query_adapter_callback,<br>    schema=\"db\",<br>    chunk_size=10000000,<br>)<br>source.TimeSeriesValues.apply_hints(<br>    table_name=\"customname_timeseriesvalues\",<br>    columns={<br>        \"TIMESTAMP\": {\"data_type\": \"timestamp\", \"precision\": 9, \"timezone\": False}<br>    },  # Preserve TIMESTAMP_NTZ on original data.<br>)<br><br>pipeline.run(<br>    source, write_disposition=\"merge\", primary_key=[\"ID\", \"TIMESTAMP\"]<br>)</pre>\n<p>The source setup here is quite ordinary except for the query adapter callback. The hint is to just to have the final table name in Snowflake as <em>customname</em>_timeseriesvalues, and the columns hint is to make sure that dlt does not interpret the TIMESTAMP column to be of type TIMESTAMP_TZ which does not match the base\u00a0data.</p>\n<p>What thequery adapter callback does here for us is quite awesome. By default, dlt ingests all of the records from the source table. However, by using query_adapter_callback, it is possible to return a custom query that dlt should use instead for fetching data. This allowed me to return my desired CTE as the final query to be run on the\u00a0source.</p>\n<p>Then, all I have to do is run the pipeline with \u201cmerge\u201d as the <a href=\"https://dlthub.com/docs/general-usage/incremental-loading#choosing-a-write-disposition\">write_disposition</a> and define my primary\u00a0keys.</p>\n<p>Finally, provided that the pipeline ran successfully, I just needed to reuse the connection string once again to perform the final\u00a0update:</p>\n<pre>with engine.begin() as conn:<br>    update_sql = f\"\"\"<br>        UPDATE A<br>        SET<br>            A.MAX_ROWVERSION_IMPORTED = CONVERT(binary(8), {rowversion_working}, 1),<br>            DATO = GETDATE()<br>        FROM db.TimeSeriesValuesDelta_SLET A<br>        WHERE [SYSTEM] = 'Snowflake_SYNC';<br>        \"\"\"<br>    result = conn.execute(sa.text(update_sql))<br>    logger.info(f\"Rows updated: {result.rowcount}\")</pre>\n<p>This completes the ingestion, and provides us with the reliable raw data that we can deliver for analytics needs.</p>\n<h3>Conclusion</h3>\n<p>I haven\u2019t used other popular ingestion tools like Fivetran, Airbyte extensively enough to know if this is doable with either of them. I am guessing that PyAirbyte could possibly do the same here\u200a\u2014\u200abut I am always curious to hear takes on tackling these sorts of problems. It certainly was not possible with our other ingestion tool of choice (Talend from\u00a0Qlik).</p>\n<p>While this isn\u2019t the simplest of our dlt pipelines, it\u2019s relatively easy to read as long as the extraction logic is documented (there is no such thing as too much documentation), and it <strong>works</strong>. Now, could I have tacked this along as a python script that just uses polars or something under the hood, and got this done? <em>Absolutely</em>. But will it be just as readable and reliable as any of our other pipelines? <em>Probably\u00a0not</em>.</p>\n<p>Every time a new requirement comes up, I am quite amazed that the dlthub team have thought about these scenarios and built possibilities for custom logic to be implemented where required. After all, it\u2019s just python code, and as long as we feed the data to dlt in a way it can work with, it can run pretty much anything. Kudos, <a href=\"https://medium.com/u/1725370bcbe1\">dlthub.com</a>!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f1f15cde8888\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["python","data-engineering","data-ingestion","sql","analytics"]},{"title":"Running dbt models on Airflow (MWAA)","pubDate":"2025-02-21 21:24:13","link":"https://medium.com/apache-airflow/running-dbt-models-on-airflow-mwaa-8f6ec9b8ed02?source=rss-e4abfb938c21------2","guid":"https://medium.com/p/8f6ec9b8ed02","author":"Arjun Anandkumar","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/494/1*hQLroPfmYjaklosADQuKww.png\"></figure><h3><strong>Premise</strong></h3>\n<p>The idea is simple: we need a way to run dbt reliably with Airflow, with as little interaction as possible from developers. A while back, it used to be run with simple bash statements, and as the number of models grew, this approach was no longer feasible: enter cosmos, a great Airflow-dbt orchestration module developed by Astronomer. Through this article, I hope to share some light on how we set up our Managed Workflows for Apache Airflow (MWAA) instances on AWS, or even, self-hosted Airflow to work with\u00a0<a href=\"https://www.astronomer.io/cosmos/\">cosmos</a>.</p>\n<h3>Why Cosmos?</h3>\n<p>We use Airflow to orchestrate a wide variety of tasks: mainly orchestrating dbt, kicking off various lambda functions, ECS tasks, maintenance tasks,\u00a0etc.</p>\n<p>We\u2019re a team of 10 data engineers and we oversee a total of about 150 DAGs today. What we mainly gain through Airflow is reproducibility, observability and transparency. We could probably just do a cron schedule on some server somewhere to execute dbt at a given time, but that would not give us the good error handling, alerting and testing functionality as we do have today. Not to mention, it would be practically un-monitorable. (is that a\u00a0term?)</p>\n<p>We could have continued using simple BashOperators to execute our dbt models, but we needed that nice interoperability and task division that dbt gave us when executing models locally, and the answer came to us in the form of Cosmos. This was something I was looking into at my previous job and I was happy to see that it was already in full-fledged use when I arrived at my current job. I have heard that it was a bit tricky to get up and running, but I don\u2019t have the finer details, unfortunately. I don\u2019t need to say more, this image is self explanatory:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CQ9pUUb-ti4inY_0lrvXaA.png\"></figure><h3><strong>Installing cosmos and setting\u00a0up</strong></h3>\n<p>Installing cosmos could range from being super-easy to somewhat frustrating. On paper, it is quite simple: create a virtual environment and install dbt and all its relevant packages within that environment. For us MWAA users, it is somewhat\u00a0tricky.</p>\n<p>First, as dbt recommends, dbt should be installed in a virtual env so as to not conflict with any of Airflow\u2019s own dependencies. All of the installations for MWAA are done within a \u201cstartup\u201d script, and the installation looks a little like\u00a0this:</p>\n<pre>#### Setup Dbt python environments<br>if ! [[ \"${MWAA_AIRFLOW_COMPONENT}\" == \"webserver\" ]]; then<br><br><br>   PLUGINS_PATH=\"/usr/local/airflow/plugins\"<br>   DBT_ENV_PATH=\"/tmp/dbt-env\"<br>   LOCAL_BIN=\"/usr/local/airflow/.local/bin\"<br><br><br>   #Virtual env<br>   python3 -m venv $DBT_ENV_PATH<br>   source $DBT_ENV_PATH/bin/activate<br>   echo \"installing uv\"<br>   pip3 install uv<br><br><br>   echo \"Installing required packages with uv\"<br>   uv pip install --find-links=${PLUGINS_PATH}/wheels -r ${PLUGINS_PATH}/dbt-requirements.txt<br><br><br>   deactivate<br><br>   # Create symlinks for dbt<br>   ln -sf $DBT_ENV_PATH/bin/dbt $LOCAL_BIN/dbt<br>fi</pre>\n<p>Where <strong>dbt-requirements.txt</strong> contains:</p>\n<pre>dbt-snowflake~=1.8.1<br>elementary-data==0.16.0<br>elementary-data[snowflake]</pre>\n<p>Something to note here is that this is not a one and done config. These are all installed every time a new worker has to be launched (as part of celery autoscaling), and these are some heavy packages and could take time. We noticed at one point, when we had these being installed without wheels that the workers took anywhere between 4\u20135 minutes to come up when the worker count had to be scaled up. To a non-Airflow-savvy developer, this just looks like Airflow tasks are stuck in\u00a0queue.</p>\n<p>By packaging the python wheels into our deployment and using uv to do the installations, we managed to slash that spin-up time to about 40 seconds\u200a\u2014\u200aa significant improvement.</p>\n<p>This completes the installation section, now to configure Cosmos. There\u2019s no real configuration required, except a connection that it uses to build the profiles.yml file\u200a\u2014\u200aokay, maybe that\u2019s not entirely true since there\u2019s a lot of customization that can be done within the cosmos Operators, but that\u2019s another topic on its own and is best explained by Astronomer themselves on their nice <a href=\"https://astronomer.github.io/astronomer-cosmos/\">documentation page</a>.</p>\n<h3><strong>Creating and Managing\u00a0DAGs</strong></h3>\n<h4>DAG Templating</h4>\n<p>While my team is very experienced with Python, we realised that without some templating, we would quickly come to a situation where the same task is handled with multiple flavours\u200a\u2014\u200amaking it harder to troubleshoot if/when something goes wrong. To make this happen, we simply work with a YAML input file where a developer defines a few things, such as a description, the datasets (inlets- more on this later), number of retries (for error handling), the layer (bronze, silver, gold), and the directory where the model is contained. Then, a nice python script that parses the YAML is run, which generates the final DAG for you. The setup is eerily similar to how popular Airflow youtuber Marc Lamberti describes it:</p>\n<a href=\"https://medium.com/media/35c111aee489024ab46301b67a259948/href\">https://medium.com/media/35c111aee489024ab46301b67a259948/href</a><p>Speaking of layers, our models are organised in adherence to the medallion architecture that was devised by Databricks. I recommend reading <a href=\"https://www.databricks.com/glossary/medallion-architecture\">this</a> brief article from Databricks to understand it a little\u00a0better.</p>\n<p>The datasets used as inlets within a dbt DAG are generated from each layer, ie, bronze, silver, gold and have to be referenced accordingly. These datasets are typically generated as a consequence of a successful run of either <strong><em>CopyFromExternalStageToSnowflakeOperator</em></strong> or <strong><em>DbtRunLocalOperator</em></strong>, but in reality it could be anything that is \u2018producing\u2019 the source as all airflow operator allows for an outlet definition, courtesy of the <a href=\"https://airflow.apache.org/docs/apache-airflow/2.1.1/_api/airflow/models/baseoperator/index.html\">BaseOperator</a>.</p>\n<h4>Sample DAG</h4>\n<p>A typical DAG (that orchestrates dbt models) looks like this. In it, you can also see how cosmos is configured in <strong><em>profile_config</em></strong>.</p>\n<pre># This DAG has been generated by the dynamic dag generator!<br><br>import pendulum<br>from datetime import timedelta<br>import os<br>from pathlib import PurePath<br>from airflow import DAG, Dataset<br>from cosmos import ProfileConfig<br>from cosmos.operators.local import DbtRunLocalOperator, DbtTestLocalOperator<br>from cosmos.profiles import SnowflakeUserPasswordProfileMapping<br>from dk_ourteam_airflow_common_package.src.dk_data_alerts import task_fail_slack_alert<br><br>env = os.getenv(\"DEPLOYMENT\")<br><br>dag_schedule = [<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_a\"<br>    ),<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_b\"<br>    ),<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_c\"<br>    ),<br>]<br><br><br>def on_failure_callback(context):<br>    slack_webhook = \"slack_ourteam\"<br>    task_fail_slack_alert(context, slack_webhook)<br><br><br>args = {<br>    \"owner\": \"ourteam\",<br>    \"retries\": 3,<br>    \"retry_delay\": timedelta(seconds=180),<br>    \"on_failure_callback\": on_failure_callback,<br>}<br><br>profile_config = ProfileConfig(<br>    profile_name=\"ourteam\",<br>    target_name=env,<br>    profile_mapping=SnowflakeUserPasswordProfileMapping(<br>        conn_id=\"snowflake_ourteam\",<br>        profile_args={\"schema\": \"gold\", \"threads\": 8},<br>    ),<br>)<br><br>with DAG(<br>    dag_id=\"gold_layer_dbt_dag\",<br>    start_date=pendulum.datetime(2024, 4, 8, tz=\"Europe/Copenhagen\"),<br>    description=\"This DAG builds x for the x domain\",<br>    schedule=dag_schedule,<br>    catchup=False,<br>    max_active_runs=1,<br>    default_args=args,<br>    tags=[\"gold\", \"another_tag\", \"dbt\"],<br>) as dag:<br>    dbt_run = DbtRunLocalOperator(<br>        profile_config=profile_config,<br>        project_dir=os.path.join(PurePath(__file__).parent.parent.parent.parent, \"dbt\"),<br>        task_id=\"dbt_run\",<br>        select=\"path:models/gold/sub_division\", # This runs all models in the given path, in a single task!<br>        install_deps=False,<br>    )<br><br>    dbt_test = DbtTestLocalOperator(<br>        profile_config=profile_config,<br>        project_dir=os.path.join(PurePath(__file__).parent.parent.parent.parent, \"dbt\"),<br>        task_id=\"dbt_test\",<br>        select=\"path:models/gold/sub_division\", # This runs all model tests in the given path, in a single task!<br>        install_deps=False,<br>    )<br><br>    dbt_run &gt;&gt; dbt_test<br></pre>\n<p>We did not go with the recommended <strong><em>DbtDag</em></strong> or <strong><em>DbtTaskGroup</em></strong> version because this allows us more flexibility in terms of scheduling. From the silver layer onwards, DbtTaskGroup was attempted, but ultimately our MWAA scaling setup became a bottleneck and we rolled back the\u00a0change.</p>\n<h3><strong>Monitoring and Troubleshooting</strong></h3>\n<p>For infrastructure monitoring, we didn\u2019t have to do much as Cloudwatch comes with some pre-defined dashboards which we were able to quickly modify to our liking. As for error alerts, we simply integrated those to slack, so any errors would instantly be sent to our monitored slack\u00a0channel.</p>\n<p>Luckily, there are typically very few errors on production, but if there are any, it is usually due to some failing dbt tests which will require investigation.</p>\n<h3><strong>Scaling and Optimisation</strong></h3>\n<p>We run MWAA on a minimum 2 worker count, which can scale up to 25 at peak loads. Typically, there are periods in the day where the workload is concentrated so we know that we do not permanently need a lot of workers running at all\u00a0times.</p>\n<p>What we have opted for is to reduce the amount of time that it takes for a new worker to come up through some cheeky optimisations, such as by packaging python wheels and using uv for the installations. I think this has been an MWAA-specific pain point, and I don\u2019t imagine Celery auto-scaling to take anywhere as long on a self-deployed setup. Even a new Kubernetes pod took only 8\u201310s to spin up at my past job, where we self-deployed Airflow on\u00a0<a href=\"https://learn.microsoft.com/en-us/azure/aks/\">AKS</a>.</p>\n<p>For those on MWAA, just be wary that Amazon has their own trickery under the hood\u200a\u2014\u200aand make sure to read the documentation about celery scaling <strong>very</strong> carefully.</p>\n<p>I came up with the idea of writing this article as we are soon about to start using dbt cloud throughout the organisation. Considering that it has its own scheduling capabilities (albeit, not as flexible), we will be moving all our dbt-specific orchestrations to dbt cloud for the sake of simplicity. So, this write-up was like saying goodbye to an old friend that served us for a good while. I\u2019d also like to say thanks to the lovely team at Cosmos who have been very helpful on the <a href=\"https://apache-airflow-slack.herokuapp.com/\">airflow slack channel</a> (join it, if you haven\u2019t already).</p>\n<p>I hope this helps in getting started with orchestrating dbt on Airflow. I\u2019d love to hear how it worked out for\u00a0you!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f6ec9b8ed02\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/running-dbt-models-on-airflow-mwaa-8f6ec9b8ed02\">Running dbt models on Airflow (MWAA)</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/494/1*hQLroPfmYjaklosADQuKww.png\"></figure><h3><strong>Premise</strong></h3>\n<p>The idea is simple: we need a way to run dbt reliably with Airflow, with as little interaction as possible from developers. A while back, it used to be run with simple bash statements, and as the number of models grew, this approach was no longer feasible: enter cosmos, a great Airflow-dbt orchestration module developed by Astronomer. Through this article, I hope to share some light on how we set up our Managed Workflows for Apache Airflow (MWAA) instances on AWS, or even, self-hosted Airflow to work with\u00a0<a href=\"https://www.astronomer.io/cosmos/\">cosmos</a>.</p>\n<h3>Why Cosmos?</h3>\n<p>We use Airflow to orchestrate a wide variety of tasks: mainly orchestrating dbt, kicking off various lambda functions, ECS tasks, maintenance tasks,\u00a0etc.</p>\n<p>We\u2019re a team of 10 data engineers and we oversee a total of about 150 DAGs today. What we mainly gain through Airflow is reproducibility, observability and transparency. We could probably just do a cron schedule on some server somewhere to execute dbt at a given time, but that would not give us the good error handling, alerting and testing functionality as we do have today. Not to mention, it would be practically un-monitorable. (is that a\u00a0term?)</p>\n<p>We could have continued using simple BashOperators to execute our dbt models, but we needed that nice interoperability and task division that dbt gave us when executing models locally, and the answer came to us in the form of Cosmos. This was something I was looking into at my previous job and I was happy to see that it was already in full-fledged use when I arrived at my current job. I have heard that it was a bit tricky to get up and running, but I don\u2019t have the finer details, unfortunately. I don\u2019t need to say more, this image is self explanatory:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CQ9pUUb-ti4inY_0lrvXaA.png\"></figure><h3><strong>Installing cosmos and setting\u00a0up</strong></h3>\n<p>Installing cosmos could range from being super-easy to somewhat frustrating. On paper, it is quite simple: create a virtual environment and install dbt and all its relevant packages within that environment. For us MWAA users, it is somewhat\u00a0tricky.</p>\n<p>First, as dbt recommends, dbt should be installed in a virtual env so as to not conflict with any of Airflow\u2019s own dependencies. All of the installations for MWAA are done within a \u201cstartup\u201d script, and the installation looks a little like\u00a0this:</p>\n<pre>#### Setup Dbt python environments<br>if ! [[ \"${MWAA_AIRFLOW_COMPONENT}\" == \"webserver\" ]]; then<br><br><br>   PLUGINS_PATH=\"/usr/local/airflow/plugins\"<br>   DBT_ENV_PATH=\"/tmp/dbt-env\"<br>   LOCAL_BIN=\"/usr/local/airflow/.local/bin\"<br><br><br>   #Virtual env<br>   python3 -m venv $DBT_ENV_PATH<br>   source $DBT_ENV_PATH/bin/activate<br>   echo \"installing uv\"<br>   pip3 install uv<br><br><br>   echo \"Installing required packages with uv\"<br>   uv pip install --find-links=${PLUGINS_PATH}/wheels -r ${PLUGINS_PATH}/dbt-requirements.txt<br><br><br>   deactivate<br><br>   # Create symlinks for dbt<br>   ln -sf $DBT_ENV_PATH/bin/dbt $LOCAL_BIN/dbt<br>fi</pre>\n<p>Where <strong>dbt-requirements.txt</strong> contains:</p>\n<pre>dbt-snowflake~=1.8.1<br>elementary-data==0.16.0<br>elementary-data[snowflake]</pre>\n<p>Something to note here is that this is not a one and done config. These are all installed every time a new worker has to be launched (as part of celery autoscaling), and these are some heavy packages and could take time. We noticed at one point, when we had these being installed without wheels that the workers took anywhere between 4\u20135 minutes to come up when the worker count had to be scaled up. To a non-Airflow-savvy developer, this just looks like Airflow tasks are stuck in\u00a0queue.</p>\n<p>By packaging the python wheels into our deployment and using uv to do the installations, we managed to slash that spin-up time to about 40 seconds\u200a\u2014\u200aa significant improvement.</p>\n<p>This completes the installation section, now to configure Cosmos. There\u2019s no real configuration required, except a connection that it uses to build the profiles.yml file\u200a\u2014\u200aokay, maybe that\u2019s not entirely true since there\u2019s a lot of customization that can be done within the cosmos Operators, but that\u2019s another topic on its own and is best explained by Astronomer themselves on their nice <a href=\"https://astronomer.github.io/astronomer-cosmos/\">documentation page</a>.</p>\n<h3><strong>Creating and Managing\u00a0DAGs</strong></h3>\n<h4>DAG Templating</h4>\n<p>While my team is very experienced with Python, we realised that without some templating, we would quickly come to a situation where the same task is handled with multiple flavours\u200a\u2014\u200amaking it harder to troubleshoot if/when something goes wrong. To make this happen, we simply work with a YAML input file where a developer defines a few things, such as a description, the datasets (inlets- more on this later), number of retries (for error handling), the layer (bronze, silver, gold), and the directory where the model is contained. Then, a nice python script that parses the YAML is run, which generates the final DAG for you. The setup is eerily similar to how popular Airflow youtuber Marc Lamberti describes it:</p>\n<a href=\"https://medium.com/media/35c111aee489024ab46301b67a259948/href\">https://medium.com/media/35c111aee489024ab46301b67a259948/href</a><p>Speaking of layers, our models are organised in adherence to the medallion architecture that was devised by Databricks. I recommend reading <a href=\"https://www.databricks.com/glossary/medallion-architecture\">this</a> brief article from Databricks to understand it a little\u00a0better.</p>\n<p>The datasets used as inlets within a dbt DAG are generated from each layer, ie, bronze, silver, gold and have to be referenced accordingly. These datasets are typically generated as a consequence of a successful run of either <strong><em>CopyFromExternalStageToSnowflakeOperator</em></strong> or <strong><em>DbtRunLocalOperator</em></strong>, but in reality it could be anything that is \u2018producing\u2019 the source as all airflow operator allows for an outlet definition, courtesy of the <a href=\"https://airflow.apache.org/docs/apache-airflow/2.1.1/_api/airflow/models/baseoperator/index.html\">BaseOperator</a>.</p>\n<h4>Sample DAG</h4>\n<p>A typical DAG (that orchestrates dbt models) looks like this. In it, you can also see how cosmos is configured in <strong><em>profile_config</em></strong>.</p>\n<pre># This DAG has been generated by the dynamic dag generator!<br><br>import pendulum<br>from datetime import timedelta<br>import os<br>from pathlib import PurePath<br>from airflow import DAG, Dataset<br>from cosmos import ProfileConfig<br>from cosmos.operators.local import DbtRunLocalOperator, DbtTestLocalOperator<br>from cosmos.profiles import SnowflakeUserPasswordProfileMapping<br>from dk_ourteam_airflow_common_package.src.dk_data_alerts import task_fail_slack_alert<br><br>env = os.getenv(\"DEPLOYMENT\")<br><br>dag_schedule = [<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_a\"<br>    ),<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_b\"<br>    ),<br>    Dataset(<br>        f\"snowflake://ourorg-identifier.us-west-1.aws/ourteam_{env}.silver.dataset_c\"<br>    ),<br>]<br><br><br>def on_failure_callback(context):<br>    slack_webhook = \"slack_ourteam\"<br>    task_fail_slack_alert(context, slack_webhook)<br><br><br>args = {<br>    \"owner\": \"ourteam\",<br>    \"retries\": 3,<br>    \"retry_delay\": timedelta(seconds=180),<br>    \"on_failure_callback\": on_failure_callback,<br>}<br><br>profile_config = ProfileConfig(<br>    profile_name=\"ourteam\",<br>    target_name=env,<br>    profile_mapping=SnowflakeUserPasswordProfileMapping(<br>        conn_id=\"snowflake_ourteam\",<br>        profile_args={\"schema\": \"gold\", \"threads\": 8},<br>    ),<br>)<br><br>with DAG(<br>    dag_id=\"gold_layer_dbt_dag\",<br>    start_date=pendulum.datetime(2024, 4, 8, tz=\"Europe/Copenhagen\"),<br>    description=\"This DAG builds x for the x domain\",<br>    schedule=dag_schedule,<br>    catchup=False,<br>    max_active_runs=1,<br>    default_args=args,<br>    tags=[\"gold\", \"another_tag\", \"dbt\"],<br>) as dag:<br>    dbt_run = DbtRunLocalOperator(<br>        profile_config=profile_config,<br>        project_dir=os.path.join(PurePath(__file__).parent.parent.parent.parent, \"dbt\"),<br>        task_id=\"dbt_run\",<br>        select=\"path:models/gold/sub_division\", # This runs all models in the given path, in a single task!<br>        install_deps=False,<br>    )<br><br>    dbt_test = DbtTestLocalOperator(<br>        profile_config=profile_config,<br>        project_dir=os.path.join(PurePath(__file__).parent.parent.parent.parent, \"dbt\"),<br>        task_id=\"dbt_test\",<br>        select=\"path:models/gold/sub_division\", # This runs all model tests in the given path, in a single task!<br>        install_deps=False,<br>    )<br><br>    dbt_run &gt;&gt; dbt_test<br></pre>\n<p>We did not go with the recommended <strong><em>DbtDag</em></strong> or <strong><em>DbtTaskGroup</em></strong> version because this allows us more flexibility in terms of scheduling. From the silver layer onwards, DbtTaskGroup was attempted, but ultimately our MWAA scaling setup became a bottleneck and we rolled back the\u00a0change.</p>\n<h3><strong>Monitoring and Troubleshooting</strong></h3>\n<p>For infrastructure monitoring, we didn\u2019t have to do much as Cloudwatch comes with some pre-defined dashboards which we were able to quickly modify to our liking. As for error alerts, we simply integrated those to slack, so any errors would instantly be sent to our monitored slack\u00a0channel.</p>\n<p>Luckily, there are typically very few errors on production, but if there are any, it is usually due to some failing dbt tests which will require investigation.</p>\n<h3><strong>Scaling and Optimisation</strong></h3>\n<p>We run MWAA on a minimum 2 worker count, which can scale up to 25 at peak loads. Typically, there are periods in the day where the workload is concentrated so we know that we do not permanently need a lot of workers running at all\u00a0times.</p>\n<p>What we have opted for is to reduce the amount of time that it takes for a new worker to come up through some cheeky optimisations, such as by packaging python wheels and using uv for the installations. I think this has been an MWAA-specific pain point, and I don\u2019t imagine Celery auto-scaling to take anywhere as long on a self-deployed setup. Even a new Kubernetes pod took only 8\u201310s to spin up at my past job, where we self-deployed Airflow on\u00a0<a href=\"https://learn.microsoft.com/en-us/azure/aks/\">AKS</a>.</p>\n<p>For those on MWAA, just be wary that Amazon has their own trickery under the hood\u200a\u2014\u200aand make sure to read the documentation about celery scaling <strong>very</strong> carefully.</p>\n<p>I came up with the idea of writing this article as we are soon about to start using dbt cloud throughout the organisation. Considering that it has its own scheduling capabilities (albeit, not as flexible), we will be moving all our dbt-specific orchestrations to dbt cloud for the sake of simplicity. So, this write-up was like saying goodbye to an old friend that served us for a good while. I\u2019d also like to say thanks to the lovely team at Cosmos who have been very helpful on the <a href=\"https://apache-airflow-slack.herokuapp.com/\">airflow slack channel</a> (join it, if you haven\u2019t already).</p>\n<p>I hope this helps in getting started with orchestrating dbt on Airflow. I\u2019d love to hear how it worked out for\u00a0you!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f6ec9b8ed02\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/running-dbt-models-on-airflow-mwaa-8f6ec9b8ed02\">Running dbt models on Airflow (MWAA)</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["cosmos","airflow","mwaa","dbt"]},{"title":"Rest API ingestions made easy with dlt","pubDate":"2024-11-28 14:28:56","link":"https://medium.com/@arjun.anand1/rest-api-ingestions-made-easy-with-dlt-d77b2bf1d50b?source=rss-e4abfb938c21------2","guid":"https://medium.com/p/d77b2bf1d50b","author":"Arjun Anandkumar","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*37HnBhPVFAdxPzXu_q1yGA.png\"><figcaption>dlthub</figcaption></figure><h3>Background</h3>\n<p>dlt. Where do I begin? When I first heard of it, the first thing I thought was: \u201cGreat, another abbreviation I had to keep in mind\u201d. But there\u2019s much more under the hood than I ever imagined.</p>\n<p>The scenario was simple, as Data Engineers, myself and several others in my team have written quite a bit of custom logic to ingest from the many REST API sources that we have. At some point, I decided that troubleshooting each of these would be a crazy amount of effort and started to explore ways to standardise how we do ingestions from these sources (and potentially many others) in the future. An opportunity came along eventually, and dlt was the first thing I looked\u00a0at.</p>\n<p>It could be <a href=\"https://dlthub.com/\">dlt</a>, or any other tool, but standardisation was the primary objective.</p>\n<p>dlt is growing in popularity, largely due to it abstracting away all the logic for extracting from an API in a reliable way and outputting it in a desired\u00a0format.</p>\n<p>The benefit of using dlt is that it super lightweight and flexible, runs pretty much anywhere and uses very limited resources (or a lot, depending on how you configure it).</p>\n<h3>Brief run of dlt\u00a0concepts</h3>\n<p>The way I see it, a dlt ingestion can be broken up into 3\u00a0parts:</p>\n<ul>\n<li>\n<a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/\">Source</a>: The source that we plan to ingest data\u00a0from.</li>\n<li>\n<a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/\">Destination</a>: The final destination that the ingested data ends up in. There are many available, but for this example, I used Snowflake.</li>\n<li>\n<a href=\"https://dlthub.com/docs/general-usage/pipeline\">Pipeline</a>: The final piece of the puzzle that defines how data is moved to the destination.</li>\n</ul>\n<h3>Pre-requisites</h3>\n<p>dlt has some pre-requisite <a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/snowflake\">configuration</a> required on Snowflake, so that it can seamlessly load data. It\u2019s not a lot, but quite well explained in their documentation so I\u2019ll save the words and just link to\u00a0it.</p>\n<p>Installing is fairly simple, just add the requirements to a file and install\u00a0with</p>\n<pre>pip install -r requirements.txt</pre>\n<p>This is in my requirements:</p>\n<pre>dlt[snowflake]==1.3.0<br>psutil<br>boto3</pre>\n<p>psutil is there because I wanted to use the \u201clog\u201d type progress parameter in my pipeline configuration. This displays detailed information about what dlt is doing, and the memory + CPU usage at these\u00a0stages.</p>\n<h3>Credentials</h3>\n<p>dlt accepts multiple methods of setting credentials:</p>\n<ul>\n<li>in\u00a0.dlt/secrets.toml</li>\n<li>environment variables</li>\n<li>python variables</li>\n</ul>\n<p>I opted to go with a combination of python variables and environment variables with the actual credentials coming from secrets manager on\u00a0AWS.</p>\n<p>dlt has a lot more information about this\u00a0<a href=\"https://dlthub.com/docs/general-usage/credentials/setup\">here</a>.</p>\n<h3>Getting started</h3>\n<p>dlt has made it easy to get started with a new pipeline. You don\u2019t need to start typing up code from scratch, rather a base template can be generated by\u00a0running:</p>\n<blockquote>dlt init &lt;source&gt; &lt;destination&gt;</blockquote>\n<p>In my\u00a0case:</p>\n<blockquote>dlt init rest_api snowflake</blockquote>\n<h3>Source configuration</h3>\n<p>So here comes the part that could be intimidating to look at, the source configuration:</p>\n<pre>source = rest_api_source(<br>    {<br>        \"client\": {<br>            \"base_url\": \"https://xxx.myapi.com/api/\",<br>            \"headers\": {<br>                \"Authorization\": f\"Basic {auth}\",<br>                \"Accept\": \"application/json\",<br>            },<br>            \"paginator\": {<br>                \"type\": \"json_response\",<br>                \"next_url_path\": \"paging.next\",<br>            },<br>        },<br>        \"resources\": [<br>            {<br>                \"name\": \"users\", # This is the name of the table that will be created in SF.<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"users\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"campaigns\", # This is the name of the table that will be created in SF.<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"campaigns\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"leads_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_data},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"leads\",<br>                    \"params\": {<br>                        \"ModifiedFrom\": start_date,<br>                        \"ModifiedTo\": end_date,<br>                    },<br>                },<br>            },<br>            {<br>                \"name\": \"calls_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>            },<br>        ],<br>    }<br>)</pre>\n<p>While this looks like a lot, it\u2019s really not. It\u2019s just a large block where one defines a base URL for the API, followed by <strong><em>resources</em></strong>, which are each of the endpoints that dlt should fetch data from, along with their individualised parameters and <em>write_disposition</em>.</p>\n<p><em>write_disposition</em> is the load strategy where replace is a full load (preferred for small datasets) and append for the larger ones where I opted for an incremental load.</p>\n<p><em>For a less intimidating example, please see </em><a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic#quick-example\"><em>dlt\u2019s own example</em></a><em> of ingesting from an example\u00a0API.</em></p>\n<h3>Pipeline + destination</h3>\n<p>This is the final, and the easiest part of the entire pipeline. There is no configuration required for the destination in terms of the pipeline (but do not forget the the preliminary steps from earlier), we simply give it a name, tell it what the destination is, give the dataset a name (this is the schema in snowflake), and finally an optional progress parameter\u200a\u2014\u200aso that things are\u00a0verbose:</p>\n<pre>pipeline = dlt.pipeline(<br>    pipeline_name=\"my_first_pipeline\",<br>    destination=\"snowflake\",<br>    dataset_name=\"bronze_first_pipeline\",  # This is the schema name in SF.<br>    progress=\"log\",  # verbose logging.<br>)</pre>\n<p>As dlt say <a href=\"https://dlthub.com/docs/pipelines/imgur/load-data-with-python-from-imgur-to-motherduck#21-adjust-the-generated-code-to-your-usecase\">themselves</a>, changing the destination to any supported ones is trivial and is done by simply changing the destination parameter in the pipeline definition above. Obviously, the required configuration for the destination needs to be in place\u00a0first.</p>\n<h3>Configuring dlt</h3>\n<p>When writing custom ingestion code, this is probably the area where I spend the most amount of time fiddling around to optimize resource usage. With dlt, it suddenly vanished because of the various configuration options available out of the\u00a0box.</p>\n<p>To keep CPU and memory usage limited, I ended up going with this in my\u00a0.dlt/config.toml file:</p>\n<pre># Adjusted based on: https://dlthub.com/docs/reference/performance<br><br>[runtime]<br>log_level=\"WARNING\"  # the system log level of dlt<br><br>###### Snowflake ######<br><br>[destination.snowflake]<br>keep_staged_files = false<br>query_tag='{{\"source\":\"{source}\", \"resource\":\"{resource}\", \"table\": \"{table}\", \"load_id\":\"{load_id}\", \"pipeline_name\":\"{pipeline_name}\"}}'<br><br>[destination]<br># https://dlthub.com/docs/general-usage/full-loading#the-staging-optimized-strategy<br>replace_strategy = \"staging-optimized\"<br><br>[load]<br>truncate_staging_dataset=true<br><br>[data_writer]<br>buffer_max_items=500<br>file_max_items=500<br><br>[extract]<br>workers=2<br><br>[normalize]<br># https://dlthub.com/docs/reference/performance#normalize<br>workers=2<br>start_method=\"spawn\"<br><br>[normalize.data_writer]<br>buffer_max_items=100<br>file_max_bytes=10000</pre>\n<p>Comments have been added where necessary to indicate why I went with each of these choices. In summary, adding these took down my peak memory usage from over 8GB for the largest data interval (about 200K records) to under 2GB simply because I was loading lesser items into the buffer and using just 2 parallel processes for the normalisation process.</p>\n<p>On the snowflake configuration side of things, I decided not to keep staged files, and add a query_tag so that we can use it for cost estimation at a later\u00a0point.</p>\n<p>As an add-on to this configuration, I also opted to use dlt\u2019s suggestion to <em>yield</em> pages instead of rows to produce data. This approach makes some processes more effective by reducing the number of necessary function calls (each chunk of data that you yield goes through the extract pipeline once, so if you yield a chunk of 10,000 items, you will gain significant savings). This is done by using the <strong><em>@dlt.resource decorator</em></strong>:</p>\n<pre>@dlt.resource<br>async def a_list_items(start, limit):<br>    # simulate a slow REST API where you wait 0.1 sec for each item<br>    index = start<br>    while index &lt; start + limit:<br>        await asyncio.sleep(0.1)<br>        yield index<br>        index += 1</pre>\n<h3>Running the\u00a0pipeline</h3>\n<p>Now that the source, destination, credentials and pipeline is configured according to our needs, there is nothing to do but to let it\u00a0run:</p>\n<pre>run = pipeline.run(source)</pre>\n<p>Provided that there are no serious issues, you should have your ingested tables ready in snowflake at the end of the pipeline!</p>\n<p>And to conclude a pipeline, we need to talk about its state. Dlt has a bunch of <a href=\"https://dlthub.com/docs/general-usage/state\">ways</a> to handle state, and the one I went with is to sync with the destination. It made the most sense to me for my pipeline, but they also warn NOT to use dlt state if it grows to some million records. I have yet to encounter that, so I\u2019ll have to figure that out when/if it\u00a0happens.</p>\n<h3>A word about normalisation</h3>\n<p>One of dlt\u2019s most powerful features is its automatic schema detection and normalisation, ie, flattening JSON objects into understandable outputs. This includes snake-casing of json field. For example: preCallLeadStatus gets output as pre_call_lead_status without any additional code.</p>\n<p>To see what schema dlt infers from the API, simply add the following parameters to the pipeline definition:</p>\n<pre>import_schema_path=\"schemas/import\",<br>export_schema_path=\"schemas/export\"</pre>\n<p>dlt will then export the inferred schema to yaml files in the provided directories, and one could go in there and adjust things to their liking for the next\u00a0run.</p>\n<p>At times, if it cannot normalise a column, it can end up being simply omitted without warning from the output. For example, if the data looks like\u00a0this:</p>\n<pre>{'campaign': {'uniqueId': 'X', 'code': 'X plus'}, 'user': {'uniqueId': 'X', 'orgCode': 'X'}, 'organizationalUnits': [{'uniqueId': 'X', 'orgCode': 'X'}, {'uniqueId': 'xxxx', 'orgCode': 'X'}, {'uniqueId': 'X', 'orgCode': 'X'}], 'uniqueId': 'X', 'uniqueLeadId': 'X', 'leadPhoneNumber': '+1', 'endCause': 'unknown', 'inProgr......}</pre>\n<p>Notice here that <strong><em>organizationalUnits</em></strong> is a list rather than a dict, and due to that, it ends up getting skipped (I haven\u2019t figured out any other reason why it would have been skipped). To avoid this from happening, and to get the list anyway, one could use json to dump it and tell dlt to preserve\u00a0it:</p>\n<pre>def ensure_org(record):<br>    import json<br>    record[\"organizationalUnits\"] = json.dumps(record.get(\"organizationalUnits\", []))<br>    return record<br><br># Updated resources<br>\"resources\": [<br>                {<br>                \"name\": \"calls_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>              }<br>            ]</pre>\n<p>As shown above, in the resource block, ensure that a <strong><em>processing_steps</em></strong> parameter is added, and to map the field, we call the <strong><em>ensure_org</em></strong> function that returns the data in the desired type. There appear to be other ways to do this, but this appears to be the easiest from what I have seen so\u00a0far.</p>\n<h3>Full example</h3>\n<p>And finally, here is a full example where I\u2019ve got a pipeline that extracts from my REST API to Snowflake which mixes both full-replace endpoints, as well as endpoints to load incrementally from:</p>\n<pre>from datetime import datetime, timedelta<br>import dlt<br>from dlt.sources.rest_api import rest_api_source<br>from helpers import get_secret<br>import asyncio<br>import os<br><br><br>start = (datetime.now() - timedelta(days=7)).replace(<br>    hour=0, minute=0, second=0, microsecond=0<br>)<br>start_date = start.isoformat()<br>end = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)<br>end_date = end.isoformat()<br>today_formatted = datetime.now().strftime(\"%Y-%m-%d\")<br><br><br>def ensure_data(record):<br>    import json<br>    # To prevent normalizing the data field in hundreds of new columns. Only a few are used<br>    record[\"data\"] = json.dumps(record.get(\"data\", []))<br>    return record<br><br><br>def ensure_org(record):<br>    import json<br>    # Ensure the `organizationalUnits` field is included as JSON<br>    record[\"organizationalUnits\"] = json.dumps(record.get(\"organizationalUnits\", []))<br>    return record<br><br><br>secret = get_secret(\"api-credential\")<br>snowflake_credentials = get_secret(\"snowflake-credential\")<br>auth = secret[\"auth_key\"]<br><br><br># Reduce memory usage by yielding results.<br># https://dlthub.com/docs/reference/performance#yield-pages-instead-of-rows<br>@dlt.resource<br>async def a_list_items(start, limit):<br>    index = start<br>    while index &lt; start + limit:<br>        await asyncio.sleep(0.1)<br>        yield index<br>        index += 1<br><br><br># Source config<br>source = rest_api_source(<br>    {<br>        \"client\": {<br>            \"base_url\": \"https://xxx.myapi.com/api/\",<br>            \"headers\": {<br>                \"Authorization\": f\"Basic {auth}\",<br>                \"Accept\": \"application/json\",<br>            },<br>            \"paginator\": {<br>                \"type\": \"json_response\",<br>                \"next_url_path\": \"paging.next\",<br>            },<br>        },<br>        \"resources\": [<br>            {<br>                \"name\": \"users\",<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"users\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"campaigns\",<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"campaigns\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"leads_raw\",<br>                \"processing_steps\": [<br>                    {\"map\": ensure_data},<br>                ],<br>                \"write_disposition\": \"append\",<br>                \"endpoint\": {<br>                    \"path\": \"simpleleads\",<br>                    \"params\": {<br>                        \"ModifiedFrom\": start_date,<br>                        \"ModifiedTo\": end_date,<br>                    },<br>                },<br>            },<br>            {<br>                \"name\": \"calls_raw\",<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\",<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>            },<br>        ],<br>    }<br>)<br><br># Pipeline definition.<br>pipeline = dlt.pipeline(<br>    pipeline_name=\"my_first_pipeline\",<br>    destination=\"snowflake\",<br>    dataset_name=\"bronze_first_pipeline\",  # This is the schema name in SF.<br>    progress=\"log\",  # Verbose logging.<br>)<br><br>if __name__ == \"__main__\":<br>    start_time = datetime.now()<br><br>    # Snowflake credentials<br>    for k, v in snowflake_credentials.items():<br>        os.environ[f\"MYAPI_PIPELINE__CREDENTIALS__{k.upper()}\"] = v<br><br>    load_info = pipeline.run(source)<br><br>    end_time = datetime.now()<br>    duration = end_time - start_time<br>    print(f\"Total execution time: {duration.total_seconds():.2f} seconds\")</pre>\n<p>This ends up creating 2 tables on each run: <strong><em>users</em></strong>, <strong><em>campaigns</em></strong>, and appending to two others: <strong><em>leads_raw</em></strong>, <strong><em>calls_raw </em></strong>in the <strong>bronze_first_pipeline</strong> schema.</p>\n<h3>Where do I run\u00a0dlt?</h3>\n<p>Let\u2019s face it, we no longer want to have a permanently running resource (an EC2 or so) that\u2019s dedicated to these pipelines. They\u2019re typically very short lived and needs to be run no more than a few times a day. Since I use AWS, the no-brainer version for me was ECS since we already have a running cluster. The other option could be Lambdas (for runs that take less than 15 minutes), but there is some <a href=\"https://dlthub.com/blog/dlt-aws-taktile-blog#caveats-to-be-aware-of-when-setting-up-dlt-on-aws-lambda\">special configuration</a> one must be aware of when using\u00a0lambdas.</p>\n<p>Dlt suggests GCP Composer too, which could in turn be translated to just any Airflow instance\u200a\u2014\u200abut I am not sure I want to go down that\u00a0route.</p>\n<h3>Issues</h3>\n<p>Nothing is perfect, and that applies for dlt too. It could be my less than 1 week\u2019s hands-on time speaking here, but I ran into a few problems initially. Some were simple to get around while others were a bit more tricky. I am down\u00a0to:</p>\n<ol>\n<li>Duplication of records:<br> I think this is mainly down to my API endpoint, but I haven\u2019t yet figured out a way to do a proper incremental load. Instead, I just end up pulling data from the past x days (currently, 7). I\u2019ve overcome this by simply adding a de-duplication step in my Airflow DAG\u2019s tasks. I\u2019m sure there\u2019s a smarter way to do this, but that will be in\u00a0v2.</li>\n<li>Memory and CPU usage are quite off what is being shown in the progress\u00a0log:</li>\n</ol>\n<p>This one was a bit annoying. Dlt\u2019s progress log indicated the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aHqY8Cgzh0zEshxQoy0M_w.png\"></figure><p>But that does not appear to be polled at real-time, in the meanwhile, <strong><em>docker stats</em></strong> showed that the memory usage was well over 1GB, and 2 CPU cores were being used. The cpu usage never budges from 0.00%\u200a\u2014\u200aif anyone knows why, I\u2019d like to know. Until then, I\u2019m taking these numbers with a grain of\u00a0salt.</p>\n<h3>Conclusion</h3>\n<p>In conclusion, from what I have seen so far, dlt is akin to Airflow for orchestration, and dbt for transformations. That\u2019s high praise, but I really think it has been built with a lot of care and consideration, and is rather simple to\u00a0use.</p>\n<p>There have been some oddities along the way, but I am sure that\u2019s down to me not having explored everything that dlt has to offer, and should be something that I figure out over\u00a0time.</p>\n<p>If your data team values transparency, repeatability and testability in your ingestion pipelines, dlt is probably the one to look at. I can\u2019t wait to try it out for more complex CDC ingestions!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d77b2bf1d50b\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*37HnBhPVFAdxPzXu_q1yGA.png\"><figcaption>dlthub</figcaption></figure><h3>Background</h3>\n<p>dlt. Where do I begin? When I first heard of it, the first thing I thought was: \u201cGreat, another abbreviation I had to keep in mind\u201d. But there\u2019s much more under the hood than I ever imagined.</p>\n<p>The scenario was simple, as Data Engineers, myself and several others in my team have written quite a bit of custom logic to ingest from the many REST API sources that we have. At some point, I decided that troubleshooting each of these would be a crazy amount of effort and started to explore ways to standardise how we do ingestions from these sources (and potentially many others) in the future. An opportunity came along eventually, and dlt was the first thing I looked\u00a0at.</p>\n<p>It could be <a href=\"https://dlthub.com/\">dlt</a>, or any other tool, but standardisation was the primary objective.</p>\n<p>dlt is growing in popularity, largely due to it abstracting away all the logic for extracting from an API in a reliable way and outputting it in a desired\u00a0format.</p>\n<p>The benefit of using dlt is that it super lightweight and flexible, runs pretty much anywhere and uses very limited resources (or a lot, depending on how you configure it).</p>\n<h3>Brief run of dlt\u00a0concepts</h3>\n<p>The way I see it, a dlt ingestion can be broken up into 3\u00a0parts:</p>\n<ul>\n<li>\n<a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/\">Source</a>: The source that we plan to ingest data\u00a0from.</li>\n<li>\n<a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/\">Destination</a>: The final destination that the ingested data ends up in. There are many available, but for this example, I used Snowflake.</li>\n<li>\n<a href=\"https://dlthub.com/docs/general-usage/pipeline\">Pipeline</a>: The final piece of the puzzle that defines how data is moved to the destination.</li>\n</ul>\n<h3>Pre-requisites</h3>\n<p>dlt has some pre-requisite <a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/snowflake\">configuration</a> required on Snowflake, so that it can seamlessly load data. It\u2019s not a lot, but quite well explained in their documentation so I\u2019ll save the words and just link to\u00a0it.</p>\n<p>Installing is fairly simple, just add the requirements to a file and install\u00a0with</p>\n<pre>pip install -r requirements.txt</pre>\n<p>This is in my requirements:</p>\n<pre>dlt[snowflake]==1.3.0<br>psutil<br>boto3</pre>\n<p>psutil is there because I wanted to use the \u201clog\u201d type progress parameter in my pipeline configuration. This displays detailed information about what dlt is doing, and the memory + CPU usage at these\u00a0stages.</p>\n<h3>Credentials</h3>\n<p>dlt accepts multiple methods of setting credentials:</p>\n<ul>\n<li>in\u00a0.dlt/secrets.toml</li>\n<li>environment variables</li>\n<li>python variables</li>\n</ul>\n<p>I opted to go with a combination of python variables and environment variables with the actual credentials coming from secrets manager on\u00a0AWS.</p>\n<p>dlt has a lot more information about this\u00a0<a href=\"https://dlthub.com/docs/general-usage/credentials/setup\">here</a>.</p>\n<h3>Getting started</h3>\n<p>dlt has made it easy to get started with a new pipeline. You don\u2019t need to start typing up code from scratch, rather a base template can be generated by\u00a0running:</p>\n<blockquote>dlt init &lt;source&gt; &lt;destination&gt;</blockquote>\n<p>In my\u00a0case:</p>\n<blockquote>dlt init rest_api snowflake</blockquote>\n<h3>Source configuration</h3>\n<p>So here comes the part that could be intimidating to look at, the source configuration:</p>\n<pre>source = rest_api_source(<br>    {<br>        \"client\": {<br>            \"base_url\": \"https://xxx.myapi.com/api/\",<br>            \"headers\": {<br>                \"Authorization\": f\"Basic {auth}\",<br>                \"Accept\": \"application/json\",<br>            },<br>            \"paginator\": {<br>                \"type\": \"json_response\",<br>                \"next_url_path\": \"paging.next\",<br>            },<br>        },<br>        \"resources\": [<br>            {<br>                \"name\": \"users\", # This is the name of the table that will be created in SF.<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"users\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"campaigns\", # This is the name of the table that will be created in SF.<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"campaigns\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"leads_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_data},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"leads\",<br>                    \"params\": {<br>                        \"ModifiedFrom\": start_date,<br>                        \"ModifiedTo\": end_date,<br>                    },<br>                },<br>            },<br>            {<br>                \"name\": \"calls_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>            },<br>        ],<br>    }<br>)</pre>\n<p>While this looks like a lot, it\u2019s really not. It\u2019s just a large block where one defines a base URL for the API, followed by <strong><em>resources</em></strong>, which are each of the endpoints that dlt should fetch data from, along with their individualised parameters and <em>write_disposition</em>.</p>\n<p><em>write_disposition</em> is the load strategy where replace is a full load (preferred for small datasets) and append for the larger ones where I opted for an incremental load.</p>\n<p><em>For a less intimidating example, please see </em><a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic#quick-example\"><em>dlt\u2019s own example</em></a><em> of ingesting from an example\u00a0API.</em></p>\n<h3>Pipeline + destination</h3>\n<p>This is the final, and the easiest part of the entire pipeline. There is no configuration required for the destination in terms of the pipeline (but do not forget the the preliminary steps from earlier), we simply give it a name, tell it what the destination is, give the dataset a name (this is the schema in snowflake), and finally an optional progress parameter\u200a\u2014\u200aso that things are\u00a0verbose:</p>\n<pre>pipeline = dlt.pipeline(<br>    pipeline_name=\"my_first_pipeline\",<br>    destination=\"snowflake\",<br>    dataset_name=\"bronze_first_pipeline\",  # This is the schema name in SF.<br>    progress=\"log\",  # verbose logging.<br>)</pre>\n<p>As dlt say <a href=\"https://dlthub.com/docs/pipelines/imgur/load-data-with-python-from-imgur-to-motherduck#21-adjust-the-generated-code-to-your-usecase\">themselves</a>, changing the destination to any supported ones is trivial and is done by simply changing the destination parameter in the pipeline definition above. Obviously, the required configuration for the destination needs to be in place\u00a0first.</p>\n<h3>Configuring dlt</h3>\n<p>When writing custom ingestion code, this is probably the area where I spend the most amount of time fiddling around to optimize resource usage. With dlt, it suddenly vanished because of the various configuration options available out of the\u00a0box.</p>\n<p>To keep CPU and memory usage limited, I ended up going with this in my\u00a0.dlt/config.toml file:</p>\n<pre># Adjusted based on: https://dlthub.com/docs/reference/performance<br><br>[runtime]<br>log_level=\"WARNING\"  # the system log level of dlt<br><br>###### Snowflake ######<br><br>[destination.snowflake]<br>keep_staged_files = false<br>query_tag='{{\"source\":\"{source}\", \"resource\":\"{resource}\", \"table\": \"{table}\", \"load_id\":\"{load_id}\", \"pipeline_name\":\"{pipeline_name}\"}}'<br><br>[destination]<br># https://dlthub.com/docs/general-usage/full-loading#the-staging-optimized-strategy<br>replace_strategy = \"staging-optimized\"<br><br>[load]<br>truncate_staging_dataset=true<br><br>[data_writer]<br>buffer_max_items=500<br>file_max_items=500<br><br>[extract]<br>workers=2<br><br>[normalize]<br># https://dlthub.com/docs/reference/performance#normalize<br>workers=2<br>start_method=\"spawn\"<br><br>[normalize.data_writer]<br>buffer_max_items=100<br>file_max_bytes=10000</pre>\n<p>Comments have been added where necessary to indicate why I went with each of these choices. In summary, adding these took down my peak memory usage from over 8GB for the largest data interval (about 200K records) to under 2GB simply because I was loading lesser items into the buffer and using just 2 parallel processes for the normalisation process.</p>\n<p>On the snowflake configuration side of things, I decided not to keep staged files, and add a query_tag so that we can use it for cost estimation at a later\u00a0point.</p>\n<p>As an add-on to this configuration, I also opted to use dlt\u2019s suggestion to <em>yield</em> pages instead of rows to produce data. This approach makes some processes more effective by reducing the number of necessary function calls (each chunk of data that you yield goes through the extract pipeline once, so if you yield a chunk of 10,000 items, you will gain significant savings). This is done by using the <strong><em>@dlt.resource decorator</em></strong>:</p>\n<pre>@dlt.resource<br>async def a_list_items(start, limit):<br>    # simulate a slow REST API where you wait 0.1 sec for each item<br>    index = start<br>    while index &lt; start + limit:<br>        await asyncio.sleep(0.1)<br>        yield index<br>        index += 1</pre>\n<h3>Running the\u00a0pipeline</h3>\n<p>Now that the source, destination, credentials and pipeline is configured according to our needs, there is nothing to do but to let it\u00a0run:</p>\n<pre>run = pipeline.run(source)</pre>\n<p>Provided that there are no serious issues, you should have your ingested tables ready in snowflake at the end of the pipeline!</p>\n<p>And to conclude a pipeline, we need to talk about its state. Dlt has a bunch of <a href=\"https://dlthub.com/docs/general-usage/state\">ways</a> to handle state, and the one I went with is to sync with the destination. It made the most sense to me for my pipeline, but they also warn NOT to use dlt state if it grows to some million records. I have yet to encounter that, so I\u2019ll have to figure that out when/if it\u00a0happens.</p>\n<h3>A word about normalisation</h3>\n<p>One of dlt\u2019s most powerful features is its automatic schema detection and normalisation, ie, flattening JSON objects into understandable outputs. This includes snake-casing of json field. For example: preCallLeadStatus gets output as pre_call_lead_status without any additional code.</p>\n<p>To see what schema dlt infers from the API, simply add the following parameters to the pipeline definition:</p>\n<pre>import_schema_path=\"schemas/import\",<br>export_schema_path=\"schemas/export\"</pre>\n<p>dlt will then export the inferred schema to yaml files in the provided directories, and one could go in there and adjust things to their liking for the next\u00a0run.</p>\n<p>At times, if it cannot normalise a column, it can end up being simply omitted without warning from the output. For example, if the data looks like\u00a0this:</p>\n<pre>{'campaign': {'uniqueId': 'X', 'code': 'X plus'}, 'user': {'uniqueId': 'X', 'orgCode': 'X'}, 'organizationalUnits': [{'uniqueId': 'X', 'orgCode': 'X'}, {'uniqueId': 'xxxx', 'orgCode': 'X'}, {'uniqueId': 'X', 'orgCode': 'X'}], 'uniqueId': 'X', 'uniqueLeadId': 'X', 'leadPhoneNumber': '+1', 'endCause': 'unknown', 'inProgr......}</pre>\n<p>Notice here that <strong><em>organizationalUnits</em></strong> is a list rather than a dict, and due to that, it ends up getting skipped (I haven\u2019t figured out any other reason why it would have been skipped). To avoid this from happening, and to get the list anyway, one could use json to dump it and tell dlt to preserve\u00a0it:</p>\n<pre>def ensure_org(record):<br>    import json<br>    record[\"organizationalUnits\"] = json.dumps(record.get(\"organizationalUnits\", []))<br>    return record<br><br># Updated resources<br>\"resources\": [<br>                {<br>                \"name\": \"calls_raw\", # This is the name of the table that will be created in SF.<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\", # Incremental<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>              }<br>            ]</pre>\n<p>As shown above, in the resource block, ensure that a <strong><em>processing_steps</em></strong> parameter is added, and to map the field, we call the <strong><em>ensure_org</em></strong> function that returns the data in the desired type. There appear to be other ways to do this, but this appears to be the easiest from what I have seen so\u00a0far.</p>\n<h3>Full example</h3>\n<p>And finally, here is a full example where I\u2019ve got a pipeline that extracts from my REST API to Snowflake which mixes both full-replace endpoints, as well as endpoints to load incrementally from:</p>\n<pre>from datetime import datetime, timedelta<br>import dlt<br>from dlt.sources.rest_api import rest_api_source<br>from helpers import get_secret<br>import asyncio<br>import os<br><br><br>start = (datetime.now() - timedelta(days=7)).replace(<br>    hour=0, minute=0, second=0, microsecond=0<br>)<br>start_date = start.isoformat()<br>end = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)<br>end_date = end.isoformat()<br>today_formatted = datetime.now().strftime(\"%Y-%m-%d\")<br><br><br>def ensure_data(record):<br>    import json<br>    # To prevent normalizing the data field in hundreds of new columns. Only a few are used<br>    record[\"data\"] = json.dumps(record.get(\"data\", []))<br>    return record<br><br><br>def ensure_org(record):<br>    import json<br>    # Ensure the `organizationalUnits` field is included as JSON<br>    record[\"organizationalUnits\"] = json.dumps(record.get(\"organizationalUnits\", []))<br>    return record<br><br><br>secret = get_secret(\"api-credential\")<br>snowflake_credentials = get_secret(\"snowflake-credential\")<br>auth = secret[\"auth_key\"]<br><br><br># Reduce memory usage by yielding results.<br># https://dlthub.com/docs/reference/performance#yield-pages-instead-of-rows<br>@dlt.resource<br>async def a_list_items(start, limit):<br>    index = start<br>    while index &lt; start + limit:<br>        await asyncio.sleep(0.1)<br>        yield index<br>        index += 1<br><br><br># Source config<br>source = rest_api_source(<br>    {<br>        \"client\": {<br>            \"base_url\": \"https://xxx.myapi.com/api/\",<br>            \"headers\": {<br>                \"Authorization\": f\"Basic {auth}\",<br>                \"Accept\": \"application/json\",<br>            },<br>            \"paginator\": {<br>                \"type\": \"json_response\",<br>                \"next_url_path\": \"paging.next\",<br>            },<br>        },<br>        \"resources\": [<br>            {<br>                \"name\": \"users\",<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"users\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"campaigns\",<br>                \"write_disposition\": \"replace\",<br>                \"endpoint\": {<br>                    \"path\": \"campaigns\",<br>                    \"params\": {\"Active\": \"All\"},<br>                },<br>            },<br>            {<br>                \"name\": \"leads_raw\",<br>                \"processing_steps\": [<br>                    {\"map\": ensure_data},<br>                ],<br>                \"write_disposition\": \"append\",<br>                \"endpoint\": {<br>                    \"path\": \"simpleleads\",<br>                    \"params\": {<br>                        \"ModifiedFrom\": start_date,<br>                        \"ModifiedTo\": end_date,<br>                    },<br>                },<br>            },<br>            {<br>                \"name\": \"calls_raw\",<br>                \"processing_steps\": [<br>                    {\"map\": ensure_org},<br>                ],<br>                \"write_disposition\": \"append\",<br>                \"endpoint\": {<br>                    \"path\": \"calls\",<br>                    \"params\": {<br>                        \"StartTime\": start_date,<br>                        \"TimeSpan\": \"7Days\",<br>                    },<br>                },<br>            },<br>        ],<br>    }<br>)<br><br># Pipeline definition.<br>pipeline = dlt.pipeline(<br>    pipeline_name=\"my_first_pipeline\",<br>    destination=\"snowflake\",<br>    dataset_name=\"bronze_first_pipeline\",  # This is the schema name in SF.<br>    progress=\"log\",  # Verbose logging.<br>)<br><br>if __name__ == \"__main__\":<br>    start_time = datetime.now()<br><br>    # Snowflake credentials<br>    for k, v in snowflake_credentials.items():<br>        os.environ[f\"MYAPI_PIPELINE__CREDENTIALS__{k.upper()}\"] = v<br><br>    load_info = pipeline.run(source)<br><br>    end_time = datetime.now()<br>    duration = end_time - start_time<br>    print(f\"Total execution time: {duration.total_seconds():.2f} seconds\")</pre>\n<p>This ends up creating 2 tables on each run: <strong><em>users</em></strong>, <strong><em>campaigns</em></strong>, and appending to two others: <strong><em>leads_raw</em></strong>, <strong><em>calls_raw </em></strong>in the <strong>bronze_first_pipeline</strong> schema.</p>\n<h3>Where do I run\u00a0dlt?</h3>\n<p>Let\u2019s face it, we no longer want to have a permanently running resource (an EC2 or so) that\u2019s dedicated to these pipelines. They\u2019re typically very short lived and needs to be run no more than a few times a day. Since I use AWS, the no-brainer version for me was ECS since we already have a running cluster. The other option could be Lambdas (for runs that take less than 15 minutes), but there is some <a href=\"https://dlthub.com/blog/dlt-aws-taktile-blog#caveats-to-be-aware-of-when-setting-up-dlt-on-aws-lambda\">special configuration</a> one must be aware of when using\u00a0lambdas.</p>\n<p>Dlt suggests GCP Composer too, which could in turn be translated to just any Airflow instance\u200a\u2014\u200abut I am not sure I want to go down that\u00a0route.</p>\n<h3>Issues</h3>\n<p>Nothing is perfect, and that applies for dlt too. It could be my less than 1 week\u2019s hands-on time speaking here, but I ran into a few problems initially. Some were simple to get around while others were a bit more tricky. I am down\u00a0to:</p>\n<ol>\n<li>Duplication of records:<br> I think this is mainly down to my API endpoint, but I haven\u2019t yet figured out a way to do a proper incremental load. Instead, I just end up pulling data from the past x days (currently, 7). I\u2019ve overcome this by simply adding a de-duplication step in my Airflow DAG\u2019s tasks. I\u2019m sure there\u2019s a smarter way to do this, but that will be in\u00a0v2.</li>\n<li>Memory and CPU usage are quite off what is being shown in the progress\u00a0log:</li>\n</ol>\n<p>This one was a bit annoying. Dlt\u2019s progress log indicated the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aHqY8Cgzh0zEshxQoy0M_w.png\"></figure><p>But that does not appear to be polled at real-time, in the meanwhile, <strong><em>docker stats</em></strong> showed that the memory usage was well over 1GB, and 2 CPU cores were being used. The cpu usage never budges from 0.00%\u200a\u2014\u200aif anyone knows why, I\u2019d like to know. Until then, I\u2019m taking these numbers with a grain of\u00a0salt.</p>\n<h3>Conclusion</h3>\n<p>In conclusion, from what I have seen so far, dlt is akin to Airflow for orchestration, and dbt for transformations. That\u2019s high praise, but I really think it has been built with a lot of care and consideration, and is rather simple to\u00a0use.</p>\n<p>There have been some oddities along the way, but I am sure that\u2019s down to me not having explored everything that dlt has to offer, and should be something that I figure out over\u00a0time.</p>\n<p>If your data team values transparency, repeatability and testability in your ingestion pipelines, dlt is probably the one to look at. I can\u2019t wait to try it out for more complex CDC ingestions!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d77b2bf1d50b\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-engineering","data-ingestion"]},{"title":"Observations from migrating away from Control-M to Airflow","pubDate":"2024-06-15 13:14:42","link":"https://medium.com/apache-airflow/observations-from-migrating-away-from-control-m-to-airflow-aa122db41326?source=rss-e4abfb938c21------2","guid":"https://medium.com/p/aa122db41326","author":"Arjun Anandkumar","thumbnail":"","description":"\n<h3><strong>Premise</strong></h3>\n<p>A couple of years ago, the company I was working at decided to move away from an expensive enterprise batch orchestration system, <a href=\"https://www.bmc.com/it-solutions/control-m.html\">Control-M</a>, to an open-source platform, Apache Airflow. For privacy reasons, I refer to the organisation as just \u201cthe company\u201d.</p>\n<p>This shift represented a big change,\u00a0because:</p>\n<p>a) Open source software wasn\u2019t really the way the company picked software due to enterprise support needs and so on<br>b) It would bring about decentralization (more on this a bit\u00a0later).</p>\n<p>To keep things in perspective, I\u2019ll just call tasks as <em>jobs</em> within this article. The majority of jobs in Control-M are so called \u2018OS\u2019 jobs, which just means that Control-M will have an agent deployed on the Windows/Linux on-premise servers which executes a script/command and reports back on its state to the main Control-M server. At least a good 95% of these OS jobs are running on Windows servers\u200a\u2014\u200asomething not immediately possible with Airflow at the time it was being considered as a replacement of Control-M.</p>\n<p>I no longer work at the company, but I have helped leave behind a solid foundation that the engineers taking over from me can build on and complete the migration journey and enable new users that need an orchestrator. When I left, we had over <strong>400 DAGs</strong> running on production with over <strong>10,000</strong> daily task executions. Airflow has slowly, but surely become an important part of the data ecosystem at the company, and it will only grow from\u00a0here.</p>\n<h4><strong>Motivation behind this\u00a0article</strong></h4>\n<p>With this article, I want to share some aspects of our deployment that allowed us to deploy scalable and reliable Airflow instances, with some trickery on top for custom Role Based Access Control (RBAC). My hope is that it will be of help for someone considering Airflow for a bit of a non-standard use case, or at least, I haven\u2019t seen anyone else attempting to do something similar. This article is a mix of the migration journey and the setup. I\u2019ll break this down into the following individual components:</p>\n<ul>\n<li>Why Airflow?</li>\n<li>Expectations and challenges</li>\n<li>Our deployment</li>\n<li>Multi-tenancy</li>\n<li>Spreding the word and Expansion</li>\n<li>Learnings</li>\n</ul>\n<h4><strong>Why Airflow?</strong></h4>\n<p>As one can imagine, the name of the game here was cost reduction because Control-M comes with a massive license fee. At the same time, there were also several benefits to be inherited such as the fact that the end-users no longer needed to wait for a specific individual or team to maintain their workflows, they could do it themselves should they want to\u200a\u2014\u200abecause everything was contained in code within git repositories. This isn\u2019t because it wasn\u2019t possible to do so from Control-M, but allowing for that means\u2026yep, you guessed it, more license\u00a0costs.</p>\n<p>It is to be noted that Control-M is a no-code drag and drop system and has been in use at the company for close to 10 years. It also has to be noted that there were quite a few users who were not familiar with Python (or any programming language for that matter), so Airflow was a hard sell for some. However, this is where a hybrid model was offered where a team would be available to take in requests to make changes to those workflows. Plus, several data engineering teams could benefit from a centrally managed Airflow instance since it meant that they would not need to take on the administrative burden. Since Azure didn\u2019t offer a managed Airflow service at the time the project was started, a managed Airflow service was out of the question.</p>\n<h4><strong>Expectations and challenges</strong></h4>\n<p>When I started at the company back in 2022, I was only told that they needed someone who knew Airflow and it was currently not in use in production due to some security challenges, which could not really be elaborated upon. Eager and interested, I took on the challenge and the task was beyond what I could\u00a0imagine.</p>\n<p>Some of the hard work had already been done. A skilled engineer had written the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-microsoft-psrp/stable/_api/airflow/providers/microsoft/psrp/operators/psrp/index.html\">PSRPOperator</a> that would enable us to execute any Windows-based (ie, the majority) jobs. Great, next, came the security\u00a0trouble.</p>\n<p>Enterprise security wasn\u2019t too fond of the idea that something <em>cloud-based</em> would be making remote calls to on-premise servers. In a few months, after many rounds of testing, security finally approved a solution where Airflow would used a Group Managed Service Account (gMSA) for each of the machines it had to perform calls to. This meant that everything could be audited and all actions of the user would be logged. There is a little more to the security design around this, but because it is something \u2018proprietary\u2019, I have to keep that information out of this write-up.</p>\n<p>With the security aspect finally being ironed out, it was time to start migrating away from Control-M to Airflow. It wasn\u2019t exactly straightforward as we soon figured that some of our potential end-users were not so excited about Airflow after all. It didn\u2019t matter to them that it was Control-M or Airflow executing their workflows, they just wanted it to run at the given schedule. I suppose this was to be expected, but after some massaging, the point and benefits became apparent and we started making good, albeit slow progress on the migrations. Slow because of the sensitive nature of many of these workloads and the many stakeholders that were involved. We could technically have just dumped everything into Airflow, however, many of these workflows lack a \u2018test\u2019 or \u2018dev\u2019 version that we could use to iron out any potential issues.</p>\n<h4><strong>Our Deployment</strong></h4>\n<p>Our deployment was in a way quite simple. We have Airflow deployed on 3 separate instances: Sandbox, TEST and PROD deployed on Azure Kubernetes Services (AKS) using the official helm-chart.</p>\n<p>For the webserver, we had some custom configuration\u200a\u2014\u200amainly to use Azure oAuth and to populate roles from the Azure app registration.</p>\n<p>For the executor, we started out with <em>Celery</em>, however at some point, we figured that a little latency for task execution is okay, and therefore we managed to get rid of two \u201calways-on\u201d components in the name of worker and redis when we switched away to using the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html\"><em>KubernetesExecutor</em></a>. Using the KubernetesExecutor brought about an approx 8\u201310s delay to start task execution, but that is a minor detail we could live with in favor of better resource utilization. Outside of some initial tweaking, administering the instances have been relatively simple.</p>\n<p>For the metadata DB we chose Postgres, as recommended.</p>\n<p>We also added a custom RBAC layer, basically a customized version of the default \u201cUser\u201d role that allowed for access to DAGs with a specific access control definition and select menu items. This was implemented as a way to allow for multi-tenancy without deploying several instances, as detailed\u00a0earlier.</p>\n<p>This is a simplified view of the architectural layout:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/839/1*v69s6C8mpGZh9aPqP6x_Gw.png\"></figure><p>We maintained a base Airflow image with our own customizations, scripts, etc. and it is built for each of our environments and stored in the respective container registries on Azure. This repository typically only saw action if there was an Airflow version upgrade, or if we had developed a new plugin,\u00a0etc.</p>\n<p>The deployment repository contained all our deployment logic, and the CI/CD pipeline that would run whenever our users pushed something to <strong>main</strong> on their own repositories. In the interest of keeping things simple for our end users, they were not exposed to this repository at all (they could of course peek, if they wanted to), and only needed to look at the pipeline run status to see if it was successful, and if any of the integrity tests had\u00a0failed.</p>\n<p>Our users were used to receiving notifications by email, so that\u2019s the alerting method we continued to offer with\u00a0Airflow.</p>\n<p>Last but not least, observability was a key requirement\u200a\u2014\u200aat least for us in the platform team. In the early days, we encountered a lot of database drops (mostly due to network bottlenecks) and we were struggling to understand when they occurred. Thankfully, we had <a href=\"https://www.splunk.com/\">Splunk</a> rolled out across the organisation around the same time, and we were able to swiftly put together good observability dashboards as well as alerts on key\u00a0metrics.</p>\n<p>We also ran a customized version of the <a href=\"https://github.com/teamclairvoyant/airflow-maintenance-dags\">maintenance DAG</a> to remove any metadata that was more than 30 days\u00a0old.</p>\n<h4>Multi-Tenancy</h4>\n<p>Decentralization was a key aspect and a major expectation from the migration to Airflow. To avoid setting up multiple instances, we began thinking of a design that would require the least amount of involvement from us, the platform team. After some trial and error, we came up with a model that was agreeable to us and most importantly, to the users. We provided 3 instances: Sandbox, Test, Production. The users were only required to add an <strong>access_control</strong> block to their DAGs with a specific role name that was created for their team. For\u00a0example:</p>\n<pre>ACCESS_CONTROL = {<br>    \"Admin-MyTeam\": {<br>        \"can_read\",<br>        \"can_edit\",<br>    }<br>}</pre>\n<p>And in the DAG definition:</p>\n<pre>access_control=ACCESS_CONTROL</pre>\n<p>These roles were mapped to Azure Active Directory groups that had the required users within them. When our CI/CD pipeline runs, the corresponding scripts that we had to fetch new roles (if any), and populate the right permissions into them. This ensured that when a user with a specific role logged into Airflow, they would see nothing but their own DAGs. This isn\u2019t true multi-tenancy by any means, but it served our objective of isolating users to their own workloads.</p>\n<p>Every team had their own DAG repos, and we simply set up triggers on our main pipeline to listen to changes on the main branch on each of the associated repositories.</p>\n<p>We used Terraform to manage the role applications on an AD group level, and these always ran as part of every deployment. This meant that the only thing that an end user had to do was to apply for an AD group assignment, and a team-member who was the owner of the group, would approve\u00a0grants.</p>\n<h4>Spreading the word and Expansion</h4>\n<p>Constantly tweaking around on the platform is never the way to work. Doing so would have meant not being able to focus on the actual task at hand, which was the migration and guiding our users. After the initial wave of troubles, we got wiser, adjusted the configuration in such a way that we caught most, if not all problems, and at some point, Airflow was just humming along to all new workloads that we kept adding to\u00a0it.</p>\n<p>We baked a lot of re-usable things into the platform such as custom timetables (to account for Danish holidays, etc.), plugins that could be useful, custom callback functions, etc. and documented them for our users to just be able to use it in their\u00a0DAGs.</p>\n<p>Then, we did a lot of hands on training, attended several company-wide show and tells to show what we had, and found quite a few users who wanted to use Airflow to orchestrate their workloads.</p>\n<p>As the number of users grew, so did the type of jobs and complexity, such as executing kubernetes jobs across various clusters in the organisation, execution of Oracle stored procedures, SSIS Packages, SAP jobs, among others. And none of these would have been possible without strong PoCs and some long knowledge sharing workshops.</p>\n<h4><strong>Learnings</strong></h4>\n<p>This could be several sub-sections on its own, however, I feel that it is best to keep things generic here. At the outset, every migration is challenging\u200a\u2014\u200ano matter the size. There are several hidden complications, and some may only appear at a crucial juncture into the migration process. But, as complex as they are, things aren\u2019t impossible to solve or work\u00a0around.</p>\n<p>As with many things, start out by understanding the needs of the users. While there were technical issues to deal with, it was more of of making engineering choices rather than actual deal breakers. A majority of our problems came in the form of offering users support, guidance and accounting for several different scenarios. But in general, by talking to and listening to our users, we were able to find a solution that fit a large majority, if not\u00a0all.</p>\n<p>Next, consider the size of team. We were a modest team of 1\u20131.5 and at most, 2 engineers doing such a massive migration at some point. Realistically, this was a hidden bottleneck because we didn\u2019t really have the engineering capacity to make some major architectural or design decisions and many things were really just invented in the moment. At many points, it did feel like a one-man army. It felt extremely rewarding to cater to so many people until a certain point, when it simply became too\u00a0much.</p>\n<p>Third, and perhaps the most important, a Technical PO with a vested interest in managing stakeholders and timely development is an absolute necessity. Having an orchestrator (pun intended) for the engineering team and stakeholders makes things a lot smoother and allows for a better glimpse of the\u00a0future.</p>\n<p>It might be worthwhile to try and offload the migration responsibility to various end-teams, this takes a lot of upfront work but it pays off as it empowers the user to work on their own and who knows, they may even end up collaborating with the core team to improve the platform.</p>\n<p>Last but not the least, follow up on latest developments in Airflow and if possible, contribute to the platform. The community is perhaps Airflow\u2019s biggest strength, so it makes perfect sense to contribute and add to that strength.</p>\n<h4>Conclusion</h4>\n<p>Ultimately, not too much technical information has been shared here\u200a\u2014\u200aand that\u2019s on purpose. There exists plenty of technical information in relevant sources, but it is not often that a migration post-mortem (or mid-mortem in this case) is done and I feel like we went through a very unique journey here that\u2019s worth sharing and hopefully inspires others to give Airflow a try as opposed to an enterprise solution.</p>\n<p>Besides, it\u2019s also my very first public write-up, and I hope it\u2019s all over the\u00a0place.</p>\n<p>Many things have not been mentioned here, and the work of so many amazing people haven\u2019t been highlighted either. Feel free to share experiences if you have attempted something similar, or have questions. I\u2019m always open to chat\u00a0\ud83d\ude03</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aa122db41326\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/observations-from-migrating-away-from-control-m-to-airflow-aa122db41326\">Observations from migrating away from Control-M to Airflow</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3><strong>Premise</strong></h3>\n<p>A couple of years ago, the company I was working at decided to move away from an expensive enterprise batch orchestration system, <a href=\"https://www.bmc.com/it-solutions/control-m.html\">Control-M</a>, to an open-source platform, Apache Airflow. For privacy reasons, I refer to the organisation as just \u201cthe company\u201d.</p>\n<p>This shift represented a big change,\u00a0because:</p>\n<p>a) Open source software wasn\u2019t really the way the company picked software due to enterprise support needs and so on<br>b) It would bring about decentralization (more on this a bit\u00a0later).</p>\n<p>To keep things in perspective, I\u2019ll just call tasks as <em>jobs</em> within this article. The majority of jobs in Control-M are so called \u2018OS\u2019 jobs, which just means that Control-M will have an agent deployed on the Windows/Linux on-premise servers which executes a script/command and reports back on its state to the main Control-M server. At least a good 95% of these OS jobs are running on Windows servers\u200a\u2014\u200asomething not immediately possible with Airflow at the time it was being considered as a replacement of Control-M.</p>\n<p>I no longer work at the company, but I have helped leave behind a solid foundation that the engineers taking over from me can build on and complete the migration journey and enable new users that need an orchestrator. When I left, we had over <strong>400 DAGs</strong> running on production with over <strong>10,000</strong> daily task executions. Airflow has slowly, but surely become an important part of the data ecosystem at the company, and it will only grow from\u00a0here.</p>\n<h4><strong>Motivation behind this\u00a0article</strong></h4>\n<p>With this article, I want to share some aspects of our deployment that allowed us to deploy scalable and reliable Airflow instances, with some trickery on top for custom Role Based Access Control (RBAC). My hope is that it will be of help for someone considering Airflow for a bit of a non-standard use case, or at least, I haven\u2019t seen anyone else attempting to do something similar. This article is a mix of the migration journey and the setup. I\u2019ll break this down into the following individual components:</p>\n<ul>\n<li>Why Airflow?</li>\n<li>Expectations and challenges</li>\n<li>Our deployment</li>\n<li>Multi-tenancy</li>\n<li>Spreding the word and Expansion</li>\n<li>Learnings</li>\n</ul>\n<h4><strong>Why Airflow?</strong></h4>\n<p>As one can imagine, the name of the game here was cost reduction because Control-M comes with a massive license fee. At the same time, there were also several benefits to be inherited such as the fact that the end-users no longer needed to wait for a specific individual or team to maintain their workflows, they could do it themselves should they want to\u200a\u2014\u200abecause everything was contained in code within git repositories. This isn\u2019t because it wasn\u2019t possible to do so from Control-M, but allowing for that means\u2026yep, you guessed it, more license\u00a0costs.</p>\n<p>It is to be noted that Control-M is a no-code drag and drop system and has been in use at the company for close to 10 years. It also has to be noted that there were quite a few users who were not familiar with Python (or any programming language for that matter), so Airflow was a hard sell for some. However, this is where a hybrid model was offered where a team would be available to take in requests to make changes to those workflows. Plus, several data engineering teams could benefit from a centrally managed Airflow instance since it meant that they would not need to take on the administrative burden. Since Azure didn\u2019t offer a managed Airflow service at the time the project was started, a managed Airflow service was out of the question.</p>\n<h4><strong>Expectations and challenges</strong></h4>\n<p>When I started at the company back in 2022, I was only told that they needed someone who knew Airflow and it was currently not in use in production due to some security challenges, which could not really be elaborated upon. Eager and interested, I took on the challenge and the task was beyond what I could\u00a0imagine.</p>\n<p>Some of the hard work had already been done. A skilled engineer had written the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-microsoft-psrp/stable/_api/airflow/providers/microsoft/psrp/operators/psrp/index.html\">PSRPOperator</a> that would enable us to execute any Windows-based (ie, the majority) jobs. Great, next, came the security\u00a0trouble.</p>\n<p>Enterprise security wasn\u2019t too fond of the idea that something <em>cloud-based</em> would be making remote calls to on-premise servers. In a few months, after many rounds of testing, security finally approved a solution where Airflow would used a Group Managed Service Account (gMSA) for each of the machines it had to perform calls to. This meant that everything could be audited and all actions of the user would be logged. There is a little more to the security design around this, but because it is something \u2018proprietary\u2019, I have to keep that information out of this write-up.</p>\n<p>With the security aspect finally being ironed out, it was time to start migrating away from Control-M to Airflow. It wasn\u2019t exactly straightforward as we soon figured that some of our potential end-users were not so excited about Airflow after all. It didn\u2019t matter to them that it was Control-M or Airflow executing their workflows, they just wanted it to run at the given schedule. I suppose this was to be expected, but after some massaging, the point and benefits became apparent and we started making good, albeit slow progress on the migrations. Slow because of the sensitive nature of many of these workloads and the many stakeholders that were involved. We could technically have just dumped everything into Airflow, however, many of these workflows lack a \u2018test\u2019 or \u2018dev\u2019 version that we could use to iron out any potential issues.</p>\n<h4><strong>Our Deployment</strong></h4>\n<p>Our deployment was in a way quite simple. We have Airflow deployed on 3 separate instances: Sandbox, TEST and PROD deployed on Azure Kubernetes Services (AKS) using the official helm-chart.</p>\n<p>For the webserver, we had some custom configuration\u200a\u2014\u200amainly to use Azure oAuth and to populate roles from the Azure app registration.</p>\n<p>For the executor, we started out with <em>Celery</em>, however at some point, we figured that a little latency for task execution is okay, and therefore we managed to get rid of two \u201calways-on\u201d components in the name of worker and redis when we switched away to using the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html\"><em>KubernetesExecutor</em></a>. Using the KubernetesExecutor brought about an approx 8\u201310s delay to start task execution, but that is a minor detail we could live with in favor of better resource utilization. Outside of some initial tweaking, administering the instances have been relatively simple.</p>\n<p>For the metadata DB we chose Postgres, as recommended.</p>\n<p>We also added a custom RBAC layer, basically a customized version of the default \u201cUser\u201d role that allowed for access to DAGs with a specific access control definition and select menu items. This was implemented as a way to allow for multi-tenancy without deploying several instances, as detailed\u00a0earlier.</p>\n<p>This is a simplified view of the architectural layout:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/839/1*v69s6C8mpGZh9aPqP6x_Gw.png\"></figure><p>We maintained a base Airflow image with our own customizations, scripts, etc. and it is built for each of our environments and stored in the respective container registries on Azure. This repository typically only saw action if there was an Airflow version upgrade, or if we had developed a new plugin,\u00a0etc.</p>\n<p>The deployment repository contained all our deployment logic, and the CI/CD pipeline that would run whenever our users pushed something to <strong>main</strong> on their own repositories. In the interest of keeping things simple for our end users, they were not exposed to this repository at all (they could of course peek, if they wanted to), and only needed to look at the pipeline run status to see if it was successful, and if any of the integrity tests had\u00a0failed.</p>\n<p>Our users were used to receiving notifications by email, so that\u2019s the alerting method we continued to offer with\u00a0Airflow.</p>\n<p>Last but not least, observability was a key requirement\u200a\u2014\u200aat least for us in the platform team. In the early days, we encountered a lot of database drops (mostly due to network bottlenecks) and we were struggling to understand when they occurred. Thankfully, we had <a href=\"https://www.splunk.com/\">Splunk</a> rolled out across the organisation around the same time, and we were able to swiftly put together good observability dashboards as well as alerts on key\u00a0metrics.</p>\n<p>We also ran a customized version of the <a href=\"https://github.com/teamclairvoyant/airflow-maintenance-dags\">maintenance DAG</a> to remove any metadata that was more than 30 days\u00a0old.</p>\n<h4>Multi-Tenancy</h4>\n<p>Decentralization was a key aspect and a major expectation from the migration to Airflow. To avoid setting up multiple instances, we began thinking of a design that would require the least amount of involvement from us, the platform team. After some trial and error, we came up with a model that was agreeable to us and most importantly, to the users. We provided 3 instances: Sandbox, Test, Production. The users were only required to add an <strong>access_control</strong> block to their DAGs with a specific role name that was created for their team. For\u00a0example:</p>\n<pre>ACCESS_CONTROL = {<br>    \"Admin-MyTeam\": {<br>        \"can_read\",<br>        \"can_edit\",<br>    }<br>}</pre>\n<p>And in the DAG definition:</p>\n<pre>access_control=ACCESS_CONTROL</pre>\n<p>These roles were mapped to Azure Active Directory groups that had the required users within them. When our CI/CD pipeline runs, the corresponding scripts that we had to fetch new roles (if any), and populate the right permissions into them. This ensured that when a user with a specific role logged into Airflow, they would see nothing but their own DAGs. This isn\u2019t true multi-tenancy by any means, but it served our objective of isolating users to their own workloads.</p>\n<p>Every team had their own DAG repos, and we simply set up triggers on our main pipeline to listen to changes on the main branch on each of the associated repositories.</p>\n<p>We used Terraform to manage the role applications on an AD group level, and these always ran as part of every deployment. This meant that the only thing that an end user had to do was to apply for an AD group assignment, and a team-member who was the owner of the group, would approve\u00a0grants.</p>\n<h4>Spreading the word and Expansion</h4>\n<p>Constantly tweaking around on the platform is never the way to work. Doing so would have meant not being able to focus on the actual task at hand, which was the migration and guiding our users. After the initial wave of troubles, we got wiser, adjusted the configuration in such a way that we caught most, if not all problems, and at some point, Airflow was just humming along to all new workloads that we kept adding to\u00a0it.</p>\n<p>We baked a lot of re-usable things into the platform such as custom timetables (to account for Danish holidays, etc.), plugins that could be useful, custom callback functions, etc. and documented them for our users to just be able to use it in their\u00a0DAGs.</p>\n<p>Then, we did a lot of hands on training, attended several company-wide show and tells to show what we had, and found quite a few users who wanted to use Airflow to orchestrate their workloads.</p>\n<p>As the number of users grew, so did the type of jobs and complexity, such as executing kubernetes jobs across various clusters in the organisation, execution of Oracle stored procedures, SSIS Packages, SAP jobs, among others. And none of these would have been possible without strong PoCs and some long knowledge sharing workshops.</p>\n<h4><strong>Learnings</strong></h4>\n<p>This could be several sub-sections on its own, however, I feel that it is best to keep things generic here. At the outset, every migration is challenging\u200a\u2014\u200ano matter the size. There are several hidden complications, and some may only appear at a crucial juncture into the migration process. But, as complex as they are, things aren\u2019t impossible to solve or work\u00a0around.</p>\n<p>As with many things, start out by understanding the needs of the users. While there were technical issues to deal with, it was more of of making engineering choices rather than actual deal breakers. A majority of our problems came in the form of offering users support, guidance and accounting for several different scenarios. But in general, by talking to and listening to our users, we were able to find a solution that fit a large majority, if not\u00a0all.</p>\n<p>Next, consider the size of team. We were a modest team of 1\u20131.5 and at most, 2 engineers doing such a massive migration at some point. Realistically, this was a hidden bottleneck because we didn\u2019t really have the engineering capacity to make some major architectural or design decisions and many things were really just invented in the moment. At many points, it did feel like a one-man army. It felt extremely rewarding to cater to so many people until a certain point, when it simply became too\u00a0much.</p>\n<p>Third, and perhaps the most important, a Technical PO with a vested interest in managing stakeholders and timely development is an absolute necessity. Having an orchestrator (pun intended) for the engineering team and stakeholders makes things a lot smoother and allows for a better glimpse of the\u00a0future.</p>\n<p>It might be worthwhile to try and offload the migration responsibility to various end-teams, this takes a lot of upfront work but it pays off as it empowers the user to work on their own and who knows, they may even end up collaborating with the core team to improve the platform.</p>\n<p>Last but not the least, follow up on latest developments in Airflow and if possible, contribute to the platform. The community is perhaps Airflow\u2019s biggest strength, so it makes perfect sense to contribute and add to that strength.</p>\n<h4>Conclusion</h4>\n<p>Ultimately, not too much technical information has been shared here\u200a\u2014\u200aand that\u2019s on purpose. There exists plenty of technical information in relevant sources, but it is not often that a migration post-mortem (or mid-mortem in this case) is done and I feel like we went through a very unique journey here that\u2019s worth sharing and hopefully inspires others to give Airflow a try as opposed to an enterprise solution.</p>\n<p>Besides, it\u2019s also my very first public write-up, and I hope it\u2019s all over the\u00a0place.</p>\n<p>Many things have not been mentioned here, and the work of so many amazing people haven\u2019t been highlighted either. Feel free to share experiences if you have attempted something similar, or have questions. I\u2019m always open to chat\u00a0\ud83d\ude03</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aa122db41326\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/observations-from-migrating-away-from-control-m-to-airflow-aa122db41326\">Observations from migrating away from Control-M to Airflow</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["airflow","kubernetes","data-engineering","workflow-automation","orchestration"]}]}